{"cells":[{"cell_type":"markdown","metadata":{"id":"5utTU63WzGd3"},"source":["# Topic Modeling\n","## *Data Science Unit 4 Sprint 1 Assignment 4*\n","\n","\n","![](https://drive.google.com/uc?export=view&id=1Aze8Xe_ZZpV22IwFNUH09T2howHmF1AK)\n","\n","[Image Credit: slides from Ben Mabey](https://speakerdeck.com/bmabey/visualizing-topic-models)\n","\n","Apply Topic Modeling to Analyze a corpus of Amazon reviews\n","\n","- Load in the Amazon Review dataset\n","- Clean the dataset \n","- Vectorize the dataset \n","- Fit a Gensim LDA topic model on Amazon Reviews\n","- Select appropriate number of topics\n","- Create some dope visualization of the topics\n","- Write a few bullets on your findings in markdown at the end"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3369,"status":"ok","timestamp":1633045683267,"user":{"displayName":"Joseph catanzarite","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmMRaw8NgoEDqzevEZ6b18iOYdEH9nWTZeaFBW=s64","userId":"16649206137414945374"},"user_tz":420},"id":"X0x7DXmr8zYE","outputId":"221278f8-7182-4ba9-b2b5-d1be90807add"},"outputs":[],"source":["# !pip install pyLDAvis==3.3.1"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3363,"status":"ok","timestamp":1633045690600,"user":{"displayName":"Joseph catanzarite","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmMRaw8NgoEDqzevEZ6b18iOYdEH9nWTZeaFBW=s64","userId":"16649206137414945374"},"user_tz":420},"id":"uQGwb9xbFKIr","outputId":"2d3e7e21-407d-4df4-848d-72ad80cfe178"},"outputs":[],"source":["# !pip install pandarallel==1.4.8"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30550,"status":"ok","timestamp":1633045373118,"user":{"displayName":"Joseph catanzarite","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmMRaw8NgoEDqzevEZ6b18iOYdEH9nWTZeaFBW=s64","userId":"16649206137414945374"},"user_tz":420},"id":"nUPIrjOB_-BR","outputId":"e9986ed7-f2aa-4bc8-aea7-dae1876356dc"},"outputs":[],"source":["# !python -m spacy download en_core_web_md"]},{"cell_type":"markdown","metadata":{"id":"isyLmOAZqYR0"},"source":["## Restart runtime!"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2104,"status":"ok","timestamp":1633045400428,"user":{"displayName":"Joseph catanzarite","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmMRaw8NgoEDqzevEZ6b18iOYdEH9nWTZeaFBW=s64","userId":"16649206137414945374"},"user_tz":420},"id":"rXrXC0mrzGd5","outputId":"7f868b3f-9038-442a-9861-743fc374bea6"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/greg/github/DS-Unit-4-Sprint-1-NLP/.env/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n","/home/greg/github/DS-Unit-4-Sprint-1-NLP/.env/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:169: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n","/home/greg/github/DS-Unit-4-Sprint-1-NLP/.env/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:286: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n","/home/greg/github/DS-Unit-4-Sprint-1-NLP/.env/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:858: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True):\n","/home/greg/github/DS-Unit-4-Sprint-1-NLP/.env/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1094: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n","/home/greg/github/DS-Unit-4-Sprint-1-NLP/.env/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1120: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  eps=np.finfo(np.float).eps, positive=False):\n","/home/greg/github/DS-Unit-4-Sprint-1-NLP/.env/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1349: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n","/home/greg/github/DS-Unit-4-Sprint-1-NLP/.env/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1590: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n","/home/greg/github/DS-Unit-4-Sprint-1-NLP/.env/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1723: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n","/home/greg/github/DS-Unit-4-Sprint-1-NLP/.env/lib/python3.8/site-packages/sklearn/utils/optimize.py:18: DeprecationWarning: Please use `line_search_wolfe2` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n","  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n","/home/greg/github/DS-Unit-4-Sprint-1-NLP/.env/lib/python3.8/site-packages/sklearn/utils/optimize.py:18: DeprecationWarning: Please use `line_search_wolfe1` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n","  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n","/home/greg/github/DS-Unit-4-Sprint-1-NLP/.env/lib/python3.8/site-packages/sklearn/decomposition/_lda.py:29: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  EPS = np.finfo(np.float).eps\n","/home/greg/github/DS-Unit-4-Sprint-1-NLP/.env/lib/python3.8/site-packages/gensim/matutils.py:24: DeprecationWarning: Please use `triu` from the `scipy.linalg` namespace, the `scipy.linalg.special_matrices` namespace is deprecated.\n","  from scipy.linalg.special_matrices import triu\n","/home/greg/github/DS-Unit-4-Sprint-1-NLP/.env/lib/python3.8/site-packages/gensim/corpora/dictionary.py:11: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n","  from collections import Mapping, defaultdict\n","/home/greg/github/DS-Unit-4-Sprint-1-NLP/.env/lib/python3.8/site-packages/gensim/models/doc2vec.py:73: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n","  from collections import namedtuple, defaultdict, Iterable\n"]}],"source":["import re\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","\n","import spacy\n","spacy.util.fix_random_seed(0)\n","\n","import pyLDAvis\n","\n","import gensim\n","import gensim.corpora as corpora\n","from gensim.utils import simple_preprocess\n","from gensim.models import CoherenceModel\n","import pyLDAvis\n","import pyLDAvis.gensim \n","\n","pyLDAvis.enable_notebook()\n","from pandarallel import pandarallel\n","\n","%matplotlib inline\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","metadata":{"id":"x9ph4ZzpzGd6"},"source":["----\n","### Load the Amazon Review corpus \n","This dataset is located in the Sprint 1 Module 1 `/data` directory. \n","\n","If the provided relative path doesn't work for you, then you'll have to provide the file path so pandas can read in the file."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5495,"status":"ok","timestamp":1633042539341,"user":{"displayName":"Joseph catanzarite","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmMRaw8NgoEDqzevEZ6b18iOYdEH9nWTZeaFBW=s64","userId":"16649206137414945374"},"user_tz":420},"id":"c_L4bKk_4E84","outputId":"2fd99623-3d8e-4deb-a76c-764e31fbe41c"},"outputs":[],"source":["# A \"brute force\" way to get the Amazon reviews dataset; just clone the Sprint 1 repo!\n","# Overkill, but it does the job!\n","# !git clone https://github.com/LambdaSchool/DS-Unit-4-Sprint-1-NLP.git"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":85499,"status":"ok","timestamp":1630270078408,"user":{"displayName":"Joseph catanzarite","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmMRaw8NgoEDqzevEZ6b18iOYdEH9nWTZeaFBW=s64","userId":"16649206137414945374"},"user_tz":420},"id":"7__zXIF_504B","outputId":"f6c28e5f-7a2b-4687-9aad-534ebefbc360"},"outputs":[],"source":["# !unzip 'Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv.zip'"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"Ov482n4pzGd6"},"outputs":[],"source":["data_path = \"./content/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv\"\n","df = pd.read_csv(data_path)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":271},"executionInfo":{"elapsed":129,"status":"ok","timestamp":1630270167063,"user":{"displayName":"Joseph catanzarite","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmMRaw8NgoEDqzevEZ6b18iOYdEH9nWTZeaFBW=s64","userId":"16649206137414945374"},"user_tz":420},"id":"-yZZEwZNzGd7","outputId":"8e9b4d24-867c-4043-9f78-4df1548748be"},"outputs":[],"source":["# df.head(2)"]},{"cell_type":"markdown","metadata":{"id":"kdwr6x8PzGd7"},"source":["----\n","\n","### Clean data\n","\n","- Create a function called `clean_data` that uses regex expressions to clean your data in preparation for the vectorizer. \n","\n","- Save the clean text data to a column in your dataframe named `clean_text`\n","\n","- Feel free to re-use old code that you have written in previous modules  "]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":123,"status":"ok","timestamp":1630270170899,"user":{"displayName":"Joseph catanzarite","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmMRaw8NgoEDqzevEZ6b18iOYdEH9nWTZeaFBW=s64","userId":"16649206137414945374"},"user_tz":420},"id":"g5cL1Amq8iT_","outputId":"5e1e4f93-dccc-4f41-fea4-03c0bd421965"},"outputs":[{"data":{"text/plain":["(28332, 24)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["df.shape # (28332, 25) rows / columns"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":132,"status":"ok","timestamp":1630270172642,"user":{"displayName":"Joseph catanzarite","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmMRaw8NgoEDqzevEZ6b18iOYdEH9nWTZeaFBW=s64","userId":"16649206137414945374"},"user_tz":420},"id":"dgW6kglx8T_l","outputId":"cd6a2c7d-62e6-46c6-b999-57c07438db84"},"outputs":[{"data":{"text/plain":["\"I've bought these before. They used to have a different design, but with the new design comes longer life! I love these, especially for the price. Definitely last as long as the name brands!\""]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["df['reviews.text'][431]"]},{"cell_type":"code","execution_count":11,"metadata":{"deletable":false,"id":"4xo_hkHKzGd7","nbgrader":{"cell_type":"code","checksum":"a2513c0ebd04cf8645fe0a1feb1e1a36","grade":false,"grade_id":"cell-fa0950cfe5ef7725","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["def clean_data(text):\n","  \"\"\"\n","  Cleans data to remove unwanted characters and punctuation.\n","  \"\"\"\n","  text = text.replace('\\\\n', ' ') # Remove new line chars, might need to make something like `\\\\r\\n`. \n","  text = re.sub('[^a-zA-Z ]', ' ', text) # Remove numbers.\n","  text = re.sub('[ ]{2,}', ' ', text) # Remove multiple white spaces.\n","  return text.lower().strip() # Might need `.lstrip().rstrip()`.\n"]},{"cell_type":"code","execution_count":12,"metadata":{"deletable":false,"id":"H693L3fJzGd8","nbgrader":{"cell_type":"code","checksum":"a6ceefe5476e71efcb3d36c3e203616f","grade":false,"grade_id":"cell-5b8e2bc0f9a745f3","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["# create a clean_text column by applying  clean_data to your text\n","df['clean_text'] = df['reviews.text'].apply(clean_data)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":["'i ve bought these before they used to have a different design but with the new design comes longer life i love these especially for the price definitely last as long as the name brands'"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["df['clean_text'][431]"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"5FMvAhkBzGd8"},"outputs":[],"source":["alphebetical_chars = [\"ABCDEFGHIJKLMNOP\"]\n","# check if any of these alphabetical chars exist in your clean chars\n","assert df.clean_text.isin(alphebetical_chars).sum() == 0, \"Did you case normalize your text inside of your clean_data function?\""]},{"cell_type":"markdown","metadata":{"id":"kzqt1ainzGd8"},"source":["------\n","\n","## Determine number of topics\n","\n","We are going to run an experiment to determine how many topics exist within the `primaryCategories` of `Electronics`. This is the largest primary category containing nearly 14K documents, so we should have plenty of data. \n","\n","Just as we did in the guided project, we'll be running a gridseach over the number of topics and scoring each model using the Coherence metric to determine which number of topics we should use. \n"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"3jrHewGbzGd9"},"outputs":[],"source":["# create a mask for docs that are in the Electronics primaryCategories - save result to `electronics_mask`\n","electronics_mask = df.primaryCategories.isin([\"Electronics\"])\n","\n","# use mask to select all the documents in the Electronics primaryCategories - save result to `df_electronics`\n","df_electronics = df[electronics_mask]"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":372},"executionInfo":{"elapsed":210,"status":"ok","timestamp":1630270183672,"user":{"displayName":"Joseph catanzarite","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmMRaw8NgoEDqzevEZ6b18iOYdEH9nWTZeaFBW=s64","userId":"16649206137414945374"},"user_tz":420},"id":"peb18zW3zGd9","outputId":"a3978c32-5d74-4361-e642-76dc927f65e9"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>dateAdded</th>\n","      <th>dateUpdated</th>\n","      <th>name</th>\n","      <th>asins</th>\n","      <th>brand</th>\n","      <th>categories</th>\n","      <th>primaryCategories</th>\n","      <th>imageURLs</th>\n","      <th>keys</th>\n","      <th>...</th>\n","      <th>reviews.doRecommend</th>\n","      <th>reviews.id</th>\n","      <th>reviews.numHelpful</th>\n","      <th>reviews.rating</th>\n","      <th>reviews.sourceURLs</th>\n","      <th>reviews.text</th>\n","      <th>reviews.title</th>\n","      <th>reviews.username</th>\n","      <th>sourceURLs</th>\n","      <th>clean_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>8343</th>\n","      <td>AVpe7nGV1cnluZ0-aG2o</td>\n","      <td>2014-10-28T11:14:38Z</td>\n","      <td>2019-04-25T09:05:28Z</td>\n","      <td>AmazonBasics Nylon CD/DVD Binder (400 Capacity)</td>\n","      <td>B00DIHVMEA,B00EZ1ZTV0</td>\n","      <td>Amazonbasics</td>\n","      <td>Audio &amp; Video Accessories,TV, Video &amp; Home Aud...</td>\n","      <td>Electronics</td>\n","      <td>http://ecx.images-amazon.com/images/I/41jQha7Z...</td>\n","      <td>amazonbasicsnyloncddvdbinder400capacity/b00ez1...</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>5</td>\n","      <td>https://www.ebay.com/itm/Amazonbasics-Nylon-Cd...</td>\n","      <td>Great case to keep everything in its place! My...</td>\n","      <td>Excellent product</td>\n","      <td>qs341_5</td>\n","      <td>https://www.ebay.com/itm/AmazonBasics-Nylon-CD...</td>\n","      <td>great case to keep everything in its place my ...</td>\n","    </tr>\n","    <tr>\n","      <th>8344</th>\n","      <td>AVpe7nGV1cnluZ0-aG2o</td>\n","      <td>2014-10-28T11:14:38Z</td>\n","      <td>2019-04-25T09:05:28Z</td>\n","      <td>AmazonBasics Nylon CD/DVD Binder (400 Capacity)</td>\n","      <td>B00DIHVMEA,B00EZ1ZTV0</td>\n","      <td>Amazonbasics</td>\n","      <td>Audio &amp; Video Accessories,TV, Video &amp; Home Aud...</td>\n","      <td>Electronics</td>\n","      <td>http://ecx.images-amazon.com/images/I/41jQha7Z...</td>\n","      <td>amazonbasicsnyloncddvdbinder400capacity/b00ez1...</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>5</td>\n","      <td>http://www.amazon.co.uk/gp/product-reviews/B00...</td>\n","      <td>After discarding and getting rid of broken cd ...</td>\n","      <td>It was a much needed storage</td>\n","      <td>Diablita</td>\n","      <td>https://www.ebay.com/itm/AmazonBasics-Nylon-CD...</td>\n","      <td>after discarding and getting rid of broken cd ...</td>\n","    </tr>\n","    <tr>\n","      <th>8345</th>\n","      <td>AVpe7nGV1cnluZ0-aG2o</td>\n","      <td>2014-10-28T11:14:38Z</td>\n","      <td>2019-04-25T09:05:28Z</td>\n","      <td>AmazonBasics Nylon CD/DVD Binder (400 Capacity)</td>\n","      <td>B00DIHVMEA,B00EZ1ZTV0</td>\n","      <td>Amazonbasics</td>\n","      <td>Audio &amp; Video Accessories,TV, Video &amp; Home Aud...</td>\n","      <td>Electronics</td>\n","      <td>http://ecx.images-amazon.com/images/I/41jQha7Z...</td>\n","      <td>amazonbasicsnyloncddvdbinder400capacity/b00ez1...</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>5</td>\n","      <td>https://www.ebay.com/itm/Amazonbasics-Nylon-Cd...</td>\n","      <td>A few dollars more, but I am boycotting amazon</td>\n","      <td>it was worth it</td>\n","      <td>coldbloodblazing</td>\n","      <td>https://www.ebay.com/itm/AmazonBasics-Nylon-CD...</td>\n","      <td>a few dollars more but i am boycotting amazon</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3 rows × 25 columns</p>\n","</div>"],"text/plain":["                        id             dateAdded           dateUpdated  \\\n","8343  AVpe7nGV1cnluZ0-aG2o  2014-10-28T11:14:38Z  2019-04-25T09:05:28Z   \n","8344  AVpe7nGV1cnluZ0-aG2o  2014-10-28T11:14:38Z  2019-04-25T09:05:28Z   \n","8345  AVpe7nGV1cnluZ0-aG2o  2014-10-28T11:14:38Z  2019-04-25T09:05:28Z   \n","\n","                                                 name                  asins  \\\n","8343  AmazonBasics Nylon CD/DVD Binder (400 Capacity)  B00DIHVMEA,B00EZ1ZTV0   \n","8344  AmazonBasics Nylon CD/DVD Binder (400 Capacity)  B00DIHVMEA,B00EZ1ZTV0   \n","8345  AmazonBasics Nylon CD/DVD Binder (400 Capacity)  B00DIHVMEA,B00EZ1ZTV0   \n","\n","             brand                                         categories  \\\n","8343  Amazonbasics  Audio & Video Accessories,TV, Video & Home Aud...   \n","8344  Amazonbasics  Audio & Video Accessories,TV, Video & Home Aud...   \n","8345  Amazonbasics  Audio & Video Accessories,TV, Video & Home Aud...   \n","\n","     primaryCategories                                          imageURLs  \\\n","8343       Electronics  http://ecx.images-amazon.com/images/I/41jQha7Z...   \n","8344       Electronics  http://ecx.images-amazon.com/images/I/41jQha7Z...   \n","8345       Electronics  http://ecx.images-amazon.com/images/I/41jQha7Z...   \n","\n","                                                   keys  ...  \\\n","8343  amazonbasicsnyloncddvdbinder400capacity/b00ez1...  ...   \n","8344  amazonbasicsnyloncddvdbinder400capacity/b00ez1...  ...   \n","8345  amazonbasicsnyloncddvdbinder400capacity/b00ez1...  ...   \n","\n","     reviews.doRecommend reviews.id reviews.numHelpful reviews.rating  \\\n","8343                 NaN        NaN                NaN              5   \n","8344                 NaN        NaN                NaN              5   \n","8345                 NaN        NaN                NaN              5   \n","\n","                                     reviews.sourceURLs  \\\n","8343  https://www.ebay.com/itm/Amazonbasics-Nylon-Cd...   \n","8344  http://www.amazon.co.uk/gp/product-reviews/B00...   \n","8345  https://www.ebay.com/itm/Amazonbasics-Nylon-Cd...   \n","\n","                                           reviews.text  \\\n","8343  Great case to keep everything in its place! My...   \n","8344  After discarding and getting rid of broken cd ...   \n","8345     A few dollars more, but I am boycotting amazon   \n","\n","                     reviews.title  reviews.username  \\\n","8343             Excellent product           qs341_5   \n","8344  It was a much needed storage          Diablita   \n","8345               it was worth it  coldbloodblazing   \n","\n","                                             sourceURLs  \\\n","8343  https://www.ebay.com/itm/AmazonBasics-Nylon-CD...   \n","8344  https://www.ebay.com/itm/AmazonBasics-Nylon-CD...   \n","8345  https://www.ebay.com/itm/AmazonBasics-Nylon-CD...   \n","\n","                                             clean_text  \n","8343  great case to keep everything in its place my ...  \n","8344  after discarding and getting rid of broken cd ...  \n","8345      a few dollars more but i am boycotting amazon  \n","\n","[3 rows x 25 columns]"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["df_electronics.head(3)"]},{"cell_type":"markdown","metadata":{"id":"FfZWF0I9zGd-"},"source":["------\n","### Tokenize your documents \n","\n","Remember that you'll need to use the [**corpora**](https://radimrehurek.com/gensim/corpora/dictionary.html) class from the Gensim library. So definitely check out the docs to learn more about this tool. There is an example on how to do this in the guided project.\n","\n","But before we can use the [**corpora**](https://radimrehurek.com/gensim/corpora/dictionary.html) class, we must first tokenize our articles. \n","\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"2rAMfVfB1PS-"},"outputs":[{"name":"stdout","output_type":"stream","text":["INFO: Pandarallel will run on 7 workers.\n","INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"]}],"source":["# identify how many processors your machine has - save the result to `n_processors`\n","n_processors = 8 \n","# subtract 1 from n_processors - save the result to `nb_workers`\n","nb_workers = 7\n","# initialize just like we did in the guided project\n","import pyarrow\n","pandarallel.initialize(progress_bar=True, nb_workers = 7)\n"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["\n","# so that the progress bars will work\n","from pandarallel import progress_bars\n","progress_bars.is_notebook_lab = lambda : True\n","# raise NotImplementedError()"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"t6pNnk3vzGd-"},"outputs":[],"source":["# load in the spaCy language model\n","nlp = spacy.load(\"en_core_web_md\")"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"05x-KJPIBQa2"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"df2f1876c9cd41dd9570ec4a30358232","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=2000), Label(value='0 / 2000'))), …"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["CPU times: user 3.26 s, sys: 691 ms, total: 3.95 s\n","Wall time: 38.5 s\n"]}],"source":["%%time\n","# create our tokens in the form of lemmas \n","df_electronics['lemmas'] = df_electronics['clean_text'].parallel_apply(lambda x: [token.lemma_ for token in nlp(x) if (token.is_stop != True) and (token.is_punct != True)])"]},{"cell_type":"markdown","metadata":{"id":"WS7vTVqnzGd_"},"source":["### Use the corpora class to prep your data for LDA\n","\n","You'll need to create the same `id2word` and `corpus` objects that we created in the guided projects. So be sure to reference the guided project notebook if you need to. "]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"7paL9zS7zGd_","nbgrader":{"cell_type":"code","checksum":"53bf811aebb39e7ed6fc592851f82f8b","grade":false,"grade_id":"cell-9d92f28649aa999e","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["# Create lemma dictionary using Dictionary - save result to `id2word`\n","id2word = \n","\n","# Create Term Document Frequency list - save result to `corpus`\n","corpus = \n","\n","# YOUR CODE HERE\n","# raise NotImplementedError()"]},{"cell_type":"markdown","metadata":{"id":"vtmHceiazGd_"},"source":["## Gridsearch the number of topics \n","\n","Just as we did in the guided project, we're going to run a `for` loop over a range of possible number of topics and then plot the `coherence_values` to determine which number of topics leads to the most sensible grouping of documents. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FBICFYD4zGeA"},"outputs":[],"source":["def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n","    \"\"\"\n","    Compute c_v coherence for various number of topics\n","\n","    Parameters:\n","    ----------\n","    dictionary : Gensim dictionary\n","    corpus : Gensim corpus\n","    texts : List of input texts\n","    limit : Max num of topics\n","\n","    Returns:\n","    -------\n","    model_list : List of LDA topic models\n","    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n","    \"\"\"\n","    coherence_values = []\n","    model_list = []\n","    for num_topics in range(start, limit, step):\n","        model = gensim.models.ldamulticore.LdaMulticore(corpus=corpus,\n","                                                        id2word=id2word,\n","                                                        num_topics=num_topics, \n","                                                        chunksize=100,\n","                                                        passes=10,\n","                                                        random_state=1234,\n","                                                        per_word_topics=True,\n","                                                        workers=2)\n","        model_list.append(model)\n","        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n","        coherence_values.append(coherencemodel.get_coherence())\n","\n","    return model_list, coherence_values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nn20KdDl2HUR"},"outputs":[],"source":["%%time\n","model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=df_electronics['lemmas'], start=2, limit=16, step=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k2kJsVLt2P5-"},"outputs":[],"source":["start=2; limit=16;  step=2;\n","x = range(start, limit, step)\n","\n","plt.figure(figsize=(20,5))\n","plt.grid()\n","plt.title(\"Coherence Score vs. Number of Topics\")\n","plt.xticks(x)\n","plt.plot(x, coherence_values, \"-o\")\n","\n","plt.xlabel(\"Num Topics\")\n","plt.ylabel(\"Coherence score\")\n","\n","plt.show();"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"1Qx7OVX-zGeB","nbgrader":{"cell_type":"code","checksum":"45f72ff929ab8b765d0f0b5f35137f0d","grade":false,"grade_id":"cell-e97661faebf1ac3c","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["# use np.argmax() to get index of largest coherence value from coherence_values - save result to `max_cohereance_val_index`\n","\n","# use `max_coherence_val_index` to index model_list for the corresponding model - save result to `lda_trained_model`\n","\n","max_coherence_val_index = \n","\n","lda_trained_model = \n","\n","# YOUR CODE HERE\n","raise NotImplementedError()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":244,"status":"ok","timestamp":1630271190181,"user":{"displayName":"Joseph catanzarite","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmMRaw8NgoEDqzevEZ6b18iOYdEH9nWTZeaFBW=s64","userId":"16649206137414945374"},"user_tz":420},"id":"7y-wokYcIg9_","outputId":"b5fabbe1-26f3-4770-a011-f892bc76a3c5"},"outputs":[],"source":["lda_trained_model"]},{"cell_type":"markdown","metadata":{"id":"LwGY4o8fzGeB"},"source":["## Use pyLDAvis to visualize your topics \n","\n","Take a look at the topic bubbles and bar chart for the terms on the right hand side.  \n","\n","- Describe the topic bubbles. \n","- Do they overlap or not? \n","- What does it mean when they overlap? \n","- What does it mean when they don't overlap?\n","- Are the terms in each topic distinct from the topics in the other topic bubbles?\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3184,"status":"ok","timestamp":1630271196635,"user":{"displayName":"Joseph catanzarite","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmMRaw8NgoEDqzevEZ6b18iOYdEH9nWTZeaFBW=s64","userId":"16649206137414945374"},"user_tz":420},"id":"9poWZ8WgItYS","outputId":"26627697-0e17-468f-e3c8-71e543e2d5f7"},"outputs":[],"source":["!pip install ipywidgets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YBqK345s2oYC"},"outputs":[],"source":["# plot your topics here -- using pyLDAvis\n","# YOUR CODE HERE\n","raise NotImplementedError()"]},{"cell_type":"markdown","metadata":{"id":"9gie-6YrzGeB"},"source":["## Create a Topic id/name dictionary \n","\n","When populating your topic id/name dictionary, use the index ordering as shown in the viz tool. \n","\n","We'll use a function to map the the viz tool index ordering with the train LDA model ordering. "]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"88vMiVwQzGeC","nbgrader":{"cell_type":"code","checksum":"fc10e9728e5dcf06baf0800c3cdb88ce","grade":false,"grade_id":"cell-4905c0c1050f0d03","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["# create a dictionary \n","# keys - use topic ids from pyLDAvis visualization \n","# values - topic names that you create \n","# save dictionary to `vis_topic_name_dict`\n","\n","vis_topic_name_dict = {}\n","# YOUR CODE HERE\n","raise NotImplementedError()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iWLsK27ZzGeC"},"outputs":[],"source":["def get_topic_id_lookup_dict(vis, vis_topic_name_dict):\n","    \"\"\"\n","    Both the starting index and the ordering of topic ids bewteen the trained LDA model \n","    and the viz tool are different. So we need to create a look up dictionary that maps \n","    the correct association between topic ids from both sources. \n","    \"\"\"\n","    # value is order of topic ids accoridng to pyLDAvis tool \n","    # key is order of topic ids according to lda model\n","    model_vis_tool_topic_id_lookup = vis.topic_coordinates.topics.to_dict()\n","\n","    # invert dictionary so that \n","    # key is order of topic ids accoridng to pyLDAvis tool \n","    # value is order of topic ids according to lda model\n","    topic_id_lookup =  {v:k for k, v in model_vis_tool_topic_id_lookup.items()}\n","    \n","    return {v:vis_topic_name_dict[k]  for k, v in topic_id_lookup.items()}"]},{"cell_type":"markdown","metadata":{"id":"tR8xm8IG4M0M"},"source":["Create a topic id/name look up dict \n","that is aligned with the index ordering of the trained LDA model"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"dcsa0f_tzGeC","nbgrader":{"cell_type":"code","checksum":"55f23aa9324a225091037f0c5daf2192","grade":false,"grade_id":"cell-d38acb7b250b4079","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["topic_name_dict = get_topic_id_lookup_dict(vis, vis_topic_name_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tbeEkAlC4WNi"},"outputs":[],"source":["topic_name_dict"]},{"cell_type":"markdown","metadata":{"id":"t4NLAlB9zGeC"},"source":["## Use the LDA Model to Assign Each Document a Topic Name\n","\n","Now that we have a topic id/name look up dict that is aligned with the index ordering of the trained LDA model, we can move forward to giving each topic a topic name. \n","\n","The function below has been given to you. However, you highly encouraged to read through it and make sure that you understand what it is doing each step of the way. In fact, a good way to do this is to copy and paste the code inside of the function into a new cell, comment out all the lines of code and line by line, uncomment the code and see the output. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"23No1SMKzGeC"},"outputs":[],"source":["def get_topic_ids_for_docs(lda_model, corpus):\n","    \n","    \"\"\"\n","    Passes a Bag-of-Words vector into a trained LDA model in order to get the topic id of that document. \n","    \n","    Parameters\n","    ----------\n","    lda_model: Gensim object\n","        Must be a trained model \n","        \n","    corpus: nested lists of tuples, \n","        i.e. [[(),(), ..., ()], [(),(), ..., ()], ..., [(),(), ..., ()]]\n","        \n","    Returns\n","    -------\n","    topic_id_list: list\n","        Contains topic ids for all document vectors in corpus \n","    \"\"\"\n","    \n","    # store topic ids for each document\n","    doc_topic_ids = []\n","\n","    # iterature through the bow vectors for each doc\n","    for doc_bow in corpus:\n","        \n","        # store the topic ids for the doc\n","        topic_ids = []\n","        # store the topic probabilities for the doc\n","        topic_probs = []\n","\n","        # list of tuples\n","        # each tuple has a topic id and the prob that the doc belongs to that topic \n","        topic_id_prob_tuples = lda_trained_model.get_document_topics(doc_bow)\n","        \n","        # iterate through the topic id/prob pairs \n","        for topic_id_prob in topic_id_prob_tuples:\n","            \n","            # index for topic id\n","            topic_id = topic_id_prob[0]\n","            # index for prob that doc belongs that the corresponding topic\n","            topic_prob = topic_id_prob[1]\n","\n","            # store all topic ids for doc\n","            topic_ids.append(topic_id)\n","            # store all topic probs for doc\n","            topic_probs.append(topic_prob)\n","\n","        # get the index for the topic that had the highest probability, for the current document \n","        max_topic_prob_ind = np.argmax(topic_probs)\n","        # get the corresponding topic id\n","        max_prob_topic_id = topic_ids[max_topic_prob_ind]\n","        # store the most probable topic id for the current document\n","        doc_topic_ids.append(max_prob_topic_id)\n","        \n","    return doc_topic_ids"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"gI94W2MvzGeD","nbgrader":{"cell_type":"code","checksum":"09f2b8ebaf696dc05187cecce8c9e251","grade":false,"grade_id":"cell-e28e8774a79ac647","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["# use get_topic_ids_for_docs to get the topic id for each doc in the corpus - save result to `doc_topic_ids`\n","doc_topic_ids = \n","\n","# create a new feature in df_electronics called topic_id using `doc_topic_ids`\n","df_electronics['topic_id'] = \n","\n","# iterate through topic_id and use the lookup dict `topic_name_dict` to assign each document a topic name\n","# save results to a new feature in df_electronics called `new_topic_name`\n","df_electronics['new_topic_name'] = df_electronics['topic_id'].apply(?)\n","\n","# YOUR CODE HERE\n","raise NotImplementedError()"]},{"cell_type":"markdown","metadata":{"id":"NZa4oKuIzGeD"},"source":["## Congratulations! You have created new topic names for your documents. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kDVAZTa23xkF"},"outputs":[],"source":["cols = [\"reviews.text\", \"new_topic_name\", \"topic_id\"]\n","df_electronics[cols].head(15)"]},{"cell_type":"markdown","metadata":{"id":"gkYnBihNzGeE"},"source":["-----\n","# Stretch Goals -- see if you can create a model to classify the reviews into the latent topics you've discovered!\n","\n","\n","- Treat `topic_id` as the `y` (target) vector and train a supervised learning model to predict the topic of each document\n","- Report your results on the Slack channel!"]},{"cell_type":"markdown","metadata":{"id":"eqY0KKxKQxbm"},"source":["## Topic Modeling References\n"]},{"cell_type":"markdown","metadata":{"id":"iKhe7u4IXr0y"},"source":["\n","- [Topic Modeling with Gensim](https://www.tutorialspoint.com/gensim/gensim_topic_modeling.htm) -- Brief Tutorial\n","\n","- [Gensim documentation](https://radimrehurek.com/gensim/index.html) -- look up gensim commands\n","\n","- [`pyLDAvis` documentation](https://pyldavis.readthedocs.io/en/latest/readme.html) -- package for visualizing LDA models\n","\n","- [Visualizing Topic Models](https://speakerdeck.com/bmabey/visualizing-topic-models) -- slides from Ben Mabey\n","\n","- [Exploring the Space of Topic Coherence Measures](https://dl.acm.org/doi/10.1145/2684822.2685324) -- Study of Topic Coherence measures\n","\n","- [Exploring Topic Coherence over many models and many topics](https://www.researchgate.net/publication/232242203_Exploring_Topic_Coherence_over_many_models_and_many_topics) -- Study of automated topic coherence measures"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"DS_414_Topic_Modeling_Assignment.ipynb","provenance":[{"file_id":"1KTNIuFlkaWNUe1LIvm05h8WMBtd34a-y","timestamp":1630264104030},{"file_id":"1crZPH84tb8O5-k018-39o6qE04eC7dZm","timestamp":1630102535129}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}
