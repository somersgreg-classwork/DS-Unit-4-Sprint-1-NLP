{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rr7YTT3AcXaN"
      },
      "source": [
        "*Unit 4, Sprint 1, Module 4*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOMBkFKn6uEK"
      },
      "source": [
        "# Topic Modeling (Prepare)\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1Aze8Xe_ZZpV22IwFNUH09T2howHmF1AK)\n",
        "\n",
        "[Image Credit: slides from Ben Mabey](https://speakerdeck.com/bmabey/visualizing-topic-models)\n",
        "\n",
        "In Module 1, we talked about summarizing your documents using just token counts. Today, we're going to learn about a much more sophisticated approach - learning 'topics' from documents. Topics are a latent structure. They are often not explicitly called out and labeled in the document corpus. So some extra work is needed to figure out the topics. Today we will learn a powerful technique for doing this (almost) automatically!\n",
        "\n",
        "> **latent**: existing but not yet developed or manifest; hidden or concealed.\n",
        "\n",
        "## Use Cases\n",
        "Primary use case: What the are your documents about? <br>\n",
        "What are some industrial applications?\n",
        "* Identifying common themes in customer reviews\n",
        "* Grouping job ads into categories \n",
        "* Sorting customer inquiries into categories so they can be routed to the appropriate department for processing.\n",
        "* Monitoring communications (Email - State Department, Google) \n",
        "\n",
        "## Learning Objectives\n",
        "*At the end of the lesson you should be able to:*\n",
        "* Part 1: Describe how a Latent Dirichlet Allocation (LDA) Model works\n",
        "* Part 2: Build a LDA Model with Gensim\n",
        "* Part 3: Interpret LDA results & Select an appropriate number of topics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVewJfGqWiH_"
      },
      "source": [
        "##1.0 Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkDpOgN3Wskv"
      },
      "source": [
        "##1.0.1 Get spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZdLPDjvWfg-",
        "outputId": "e91e756b-f02c-45b4-a8c6-605d9881cd3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-md==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.2.0/en_core_web_md-3.2.0-py3-none-any.whl (45.7 MB)\n",
            "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in c:\\users\\somer\\miniconda3\\envs\\deeplearning\\lib\\site-packages (from en-core-web-md==3.2.0) (3.2.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\somer\\miniconda3\\envs\\deeplearning\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (4.63.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\somer\\miniconda3\\envs\\deeplearning\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.27.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\somer\\miniconda3\\envs\\deeplearning\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.0.1)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\somer\\miniconda3\\envs\\deeplearning\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (21.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\somer\\miniconda3\\envs\\deeplearning\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.21.5)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\somer\\miniconda3\\envs\\deeplearning\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (8.0.13)\n",
            "Requirement already satisfied: setuptools in c:\\users\\somer\\miniconda3\\envs\\deeplearning\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (58.0.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\somer\\miniconda3\\envs\\deeplearning\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.9)\n",
            "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\somer\\miniconda3\\envs\\deeplearning\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\somer\\miniconda3\\envs\\deeplearning\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\somer\\miniconda3\\envs\\deeplearning\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.4.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\somer\\miniconda3\\envs\\deeplearning\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.9.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\somer\\miniconda3\\envs\\deeplearning\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.7.6)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\somer\\miniconda3\\envs\\deeplearning\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\somer\\miniconda3\\envs\\deeplearning\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\somer\\miniconda3\\envs\\deeplearning\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.0.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\somer\\miniconda3\\envs\\deeplearning\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\somer\\miniconda3\\envs\\deeplearning\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\somer\\miniconda3\\envs\\deeplearning\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\somer\\miniconda3\\envs\\deeplearning\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\somer\\miniconda3\\envs\\deeplearning\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\somer\\miniconda3\\envs\\deeplearning\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\somer\\miniconda3\\envs\\deeplearning\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (4.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\somer\\miniconda3\\envs\\deeplearning\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\somer\\miniconda3\\envs\\deeplearning\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.11)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\somer\\miniconda3\\envs\\deeplearning\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.26.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\somer\\miniconda3\\envs\\deeplearning\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\somer\\miniconda3\\envs\\deeplearning\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.4.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\somer\\miniconda3\\envs\\deeplearning\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\somer\\miniconda3\\envs\\deeplearning\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.1.0)\n",
            "✔ Download and installation successful\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-03-23 22:19:44.596823: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
            "2022-03-23 22:19:44.597303: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
          ]
        }
      ],
      "source": [
        "# could use *web_lg or *web_sm instead\n",
        "!python -m spacy download en_core_web_md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5qOu0H6W33y"
      },
      "source": [
        "## 1.0.1 Restart runtime!\n",
        "On Colab, we need to restart runtime after this step, <br>\n",
        "or else Colab won't find spacy!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jz_KxsK4WxL1"
      },
      "source": [
        "##1.0.2 Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tw3GjX0eViA5"
      },
      "outputs": [],
      "source": [
        "# Dependencies for the week (instead of conda)\n",
        "# Run if you're using colab, otherwise you should have a local copy of the data\n",
        "# !wget https://raw.githubusercontent.com/LambdaSchool/DS-Unit-4-Sprint-1-NLP/main/requirements.txt\n",
        "# !pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnFYpOSJXEiF"
      },
      "source": [
        "##1.0.3 Imports and such"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rnyNg8sQ6uEL"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandarallel in c:\\users\\somer\\miniconda3\\envs\\deeplearning\\lib\\site-packages (1.5.7)\n",
            "Requirement already satisfied: pandas>=1 in c:\\users\\somer\\miniconda3\\envs\\deeplearning\\lib\\site-packages (from pandarallel) (1.4.1)\n",
            "Requirement already satisfied: dill>=0.3.1 in c:\\users\\somer\\miniconda3\\envs\\deeplearning\\lib\\site-packages (from pandarallel) (0.3.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\somer\\miniconda3\\envs\\deeplearning\\lib\\site-packages (from pandas>=1->pandarallel) (2021.3)\n",
            "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\somer\\miniconda3\\envs\\deeplearning\\lib\\site-packages (from pandas>=1->pandarallel) (1.21.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\somer\\miniconda3\\envs\\deeplearning\\lib\\site-packages (from pandas>=1->pandarallel) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\somer\\miniconda3\\envs\\deeplearning\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1->pandarallel) (1.16.0)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "cannot find context for 'fork'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\somer\\github\\DS-Unit-4-Sprint-1-NLP\\module4-topic-modeling\\DSPT15_414_Topic_Modeling_Lecture.ipynb Cell 12'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/somer/github/DS-Unit-4-Sprint-1-NLP/module4-topic-modeling/DSPT15_414_Topic_Modeling_Lecture.ipynb#ch0000011?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m fetch_20newsgroups\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/somer/github/DS-Unit-4-Sprint-1-NLP/module4-topic-modeling/DSPT15_414_Topic_Modeling_Lecture.ipynb#ch0000011?line=1'>2</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mpip install pandarallel\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/somer/github/DS-Unit-4-Sprint-1-NLP/module4-topic-modeling/DSPT15_414_Topic_Modeling_Lecture.ipynb#ch0000011?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandarallel\u001b[39;00m \u001b[39mimport\u001b[39;00m pandarallel\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/somer/github/DS-Unit-4-Sprint-1-NLP/module4-topic-modeling/DSPT15_414_Topic_Modeling_Lecture.ipynb#ch0000011?line=5'>6</a>\u001b[0m pandarallel\u001b[39m.\u001b[39minitialize(progress_bar\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/somer/github/DS-Unit-4-Sprint-1-NLP/module4-topic-modeling/DSPT15_414_Topic_Modeling_Lecture.ipynb#ch0000011?line=7'>8</a>\u001b[0m \u001b[39m# df.apply(func)\u001b[39;00m\n",
            "File \u001b[1;32m~\\miniconda3\\envs\\DeepLearning\\lib\\site-packages\\pandarallel\\__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='file:///c%3A/Users/somer/miniconda3/envs/DeepLearning/lib/site-packages/pandarallel/__init__.py?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m pandarallel\n\u001b[0;32m      <a href='file:///c%3A/Users/somer/miniconda3/envs/DeepLearning/lib/site-packages/pandarallel/__init__.py?line=2'>3</a>\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1.5.7\u001b[39m\u001b[39m\"\u001b[39m\n",
            "File \u001b[1;32m~\\miniconda3\\envs\\DeepLearning\\lib\\site-packages\\pandarallel\\core.py:29\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/somer/miniconda3/envs/DeepLearning/lib/site-packages/pandarallel/core.py?line=24'>25</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m WorkerStatus\n\u001b[0;32m     <a href='file:///c%3A/Users/somer/miniconda3/envs/DeepLearning/lib/site-packages/pandarallel/core.py?line=26'>27</a>\u001b[0m \u001b[39m# Python 3.8 on MacOS by default uses \"spawn\" instead of \"fork\" as start method for new\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/somer/miniconda3/envs/DeepLearning/lib/site-packages/pandarallel/core.py?line=27'>28</a>\u001b[0m \u001b[39m# processes, which is incompatible with pandarallel. We force it to use \"fork\" method.\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Users/somer/miniconda3/envs/DeepLearning/lib/site-packages/pandarallel/core.py?line=28'>29</a>\u001b[0m CONTEXT \u001b[39m=\u001b[39m multiprocessing\u001b[39m.\u001b[39;49mget_context(\u001b[39m\"\u001b[39;49m\u001b[39mfork\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     <a href='file:///c%3A/Users/somer/miniconda3/envs/DeepLearning/lib/site-packages/pandarallel/core.py?line=30'>31</a>\u001b[0m \u001b[39m# Root of Memory File System\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/somer/miniconda3/envs/DeepLearning/lib/site-packages/pandarallel/core.py?line=31'>32</a>\u001b[0m MEMORY_FS_ROOT \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/dev/shm\u001b[39m\u001b[39m\"\u001b[39m\n",
            "File \u001b[1;32m~\\miniconda3\\envs\\DeepLearning\\lib\\multiprocessing\\context.py:239\u001b[0m, in \u001b[0;36mDefaultContext.get_context\u001b[1;34m(self, method)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/somer/miniconda3/envs/DeepLearning/lib/multiprocessing/context.py?line=236'>237</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_actual_context\n\u001b[0;32m    <a href='file:///c%3A/Users/somer/miniconda3/envs/DeepLearning/lib/multiprocessing/context.py?line=237'>238</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/somer/miniconda3/envs/DeepLearning/lib/multiprocessing/context.py?line=238'>239</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mget_context(method)\n",
            "File \u001b[1;32m~\\miniconda3\\envs\\DeepLearning\\lib\\multiprocessing\\context.py:193\u001b[0m, in \u001b[0;36mBaseContext.get_context\u001b[1;34m(self, method)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/somer/miniconda3/envs/DeepLearning/lib/multiprocessing/context.py?line=190'>191</a>\u001b[0m     ctx \u001b[39m=\u001b[39m _concrete_contexts[method]\n\u001b[0;32m    <a href='file:///c%3A/Users/somer/miniconda3/envs/DeepLearning/lib/multiprocessing/context.py?line=191'>192</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/somer/miniconda3/envs/DeepLearning/lib/multiprocessing/context.py?line=192'>193</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mcannot find context for \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m method) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/somer/miniconda3/envs/DeepLearning/lib/multiprocessing/context.py?line=193'>194</a>\u001b[0m ctx\u001b[39m.\u001b[39m_check_available()\n\u001b[0;32m    <a href='file:///c%3A/Users/somer/miniconda3/envs/DeepLearning/lib/multiprocessing/context.py?line=194'>195</a>\u001b[0m \u001b[39mreturn\u001b[39;00m ctx\n",
            "\u001b[1;31mValueError\u001b[0m: cannot find context for 'fork'"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "!pip install pandarallel \n",
        "\n",
        "from pandarallel import pandarallel\n",
        "\n",
        "pandarallel.initialize(progress_bar=True)\n",
        "\n",
        "# df.apply(func)\n",
        "df.parallel_apply(func)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import spacy\n",
        "spacy.util.fix_random_seed(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbZjG6U86uEe"
      },
      "source": [
        "# Part 1: Building a Latent Dirichlet Allocation (LDA) Topic Model with Gensim\n",
        "\n",
        "Our goal in this module is to get hands-on experience in how to build and train a Latent Dirichlet Allocation topic model using the **Gensim** library. \n",
        "\n",
        "## 1.1 Latent Dirichlet Allocation (LDA) \n",
        "is a more sophisticated method of topic modelling than Latent Semantic Indexing (LSI) which we used in Module 2. Because of its complexity, a full discussion of the machinery of LDA is beyond the scope of Unit 4. We'll focus on a high level of understanding of Latent Dirichlet Allocation, meaning we'll emphasize \"what it does\" rather than \"how it does it\". \n",
        "\n",
        "LDA takes as input the corpora of documents and the number of topics that you want to identify. LDA models topics as probability distributions over words, and it models documents as probability distributions over topics.\n",
        "\n",
        "LDA is a generative probabilistic model. That means it can generate documents from probability distributions. Given an article length (number of words), an assumed probability distribution of topics, and an assumed distribution of words for each topic, LDA can generate an article, in the form of a bag of words. LDA works by starting with random distributions of words over topics and  topics over documents, then iteratively adjust these probability distributions until the generated documents are similar in structure to the input documents.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EoWE7MpVKkz"
      },
      "source": [
        "### 1.1.1 Resources for LDA\n",
        "\n",
        "[**Your Guide to Latent Dirichlet Allocation**](https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d) is a highly recommended Medium article that works through an example of LDA in elementary terms\n",
        "\n",
        "[**LDA Topic Modeling**](https://lettier.com/projects/lda-topic-modeling/) is an interactive data visualization tool -- created by the author of the above article -- that allows us to explore a simple and visual example of LDA.\n",
        "\n",
        "[**Topic Modeling with Gensim**](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/) disccusses an example implementing LDA using the same dataset that we are using in this guided project.  \n",
        "\n",
        "Luis Serrano's video [**Latent Dirichlet Allocation**](https://youtu.be/T05t-SqKArY) does a great job of breaking down and explaining LDA in an understandable way.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BDAv24nQSsx"
      },
      "source": [
        "### 1.1.2 Further discussion of LDA (Optional)\n",
        "\n",
        "LDA is a [**Probabilistic Graphical Model (PGM)**](https://en.wikipedia.org/wiki/Graphical_model)\n",
        "\n",
        "A PGM can be represented by a graph that expresses the conditional dependence structure between random variables. Here's a graphical representation of the LDA model: \n",
        "\n",
        "![](https://filebox.ece.vt.edu/~s14ece6504/projects/alfadda_topic/main_figure_3.png)\n",
        "\n",
        "The image shows the hierarchical dependency between probability distributions and their parameters. This is an application of Hierarchical Bayesian Modeling. \n",
        "\n",
        "In order to understand how LDA works, one must first understand how PGMs work, which is beyond the scope of Unit 4. If this is something that you're interested in learning more about, here are some resources: \n",
        "\n",
        "This Github repo that has transformed a textbook in a collection of Jupyter Notebooks. This repo is called [**\"Bayesian Methods for Hackers\"**](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers) <br>\n",
        "The cool thing about this repo is that each chapter covers the same material using four popular python probabilistic modeling libraries: **PyMC2, PyMC3, Pyro, and Tensorflow Probability.** So you can choose your preferred library.\n",
        "\n",
        "[**Pyro**](https://pyro.ai/) is considered a very powerful probabilistic programming library that even combines probabilistic programming with deep learning. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQQm9Q5uSVmu"
      },
      "source": [
        "## 1.2 Preparing our text data set for LDA topic modeling with `gensim`\n",
        "For this guided project we'll work with the `20newsgroups` data set that is  familiar from Module 3, but we'll extract articles from a different set of categories.<br><br>\n",
        "`gensim` is a Natural Language Processing Library, specialized for Topic Modeling applications. The two main inputs required for a `gensim` LDA topic model are the dictionary (`id2word`) and the `corpus`.<br><br>\n",
        "`id2word` is a special object created by `gensim` that keeps track of the mapping from text to numerical index, and the mapping from numerical index back to text <br><br>\n",
        "The `corpus` is a specially formatted list containing information about each document. Though its format is different, is essentially equivalent to the document-term matrix that we used with `sklearn`.<br>\n",
        "\n",
        "In this section, we will first download and clean the raw text documents, then use `spacy` to tokenize and lemmatize them. Then we'll use `gensim` to create the `id2word` and `corpus` objects that we need for topic modeling with `gensim`<br>\n",
        "\n",
        "In Part 2, we'll build and train the LDA topic model in `gensim`.\n",
        "\n",
        "References: \n",
        "- [Gensim Tutorial – A Complete Beginners Guide](https://www.machinelearningplus.com/nlp/gensim-tutorial/)\n",
        "- [Topic Modeling with Gensim (Python)](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DKwP9e1VKk0"
      },
      "source": [
        "# Problem Statement\n",
        "\n",
        "\n",
        "\n",
        "We'll extract articles belonging to the following four topics (as assigned in the 20newsgroups database)\n",
        "\n",
        "    sci\n",
        "        \\_ electronics, space\n",
        "\n",
        "\n",
        "    talk\n",
        "        \\_politics \n",
        "                  \\_ guns, middle east\n",
        "              \n",
        "What's the best way to categorize these emails - is it between science and talk? \n",
        "\n",
        "Or it between electronics, space, guns, and the Middle East? \n",
        "\n",
        "The Middle East is a pretty broad topic in and of itself, should that topic be broken down into further sub-topics?\n",
        "\n",
        "Let's learn about Topic Modeling and how it can help us answer these questions!\n",
        "\n",
        "### Load Email Corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1S6oYNoVKk0"
      },
      "outputs": [],
      "source": [
        "# notice that the categories are hierarchical\n",
        "# so there is a sense in which we have 2 topics, but also as many as 4 topics  \n",
        "categories = ['sci.electronics', 'sci.space', \n",
        "              'talk.politics.guns', 'talk.politics.mideast']\n",
        "data = fetch_20newsgroups(subset='all', \n",
        "                          remove=('headers', 'footers', 'quotes'),\n",
        "                          random_state=42, shuffle=True,\n",
        "                          categories=categories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcoCMzdZOvCF"
      },
      "outputs": [],
      "source": [
        "dir(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dwck_6IGVKk1",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-3240b1c818e9dcc2",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# create X and y from data, as we did in the Module 3 Lecture notebook\n",
        "\n",
        "###BEGIN SOLUTION\n",
        "X = data.data\n",
        "y = data.target\n",
        "target_names = data.target_names\n",
        "###END SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDyRewBO0jxV"
      },
      "source": [
        "What are the possible targets (labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYlSl1Wd0ffZ"
      },
      "outputs": [],
      "source": [
        "np.unique(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2hCP-jl0rTM"
      },
      "source": [
        "What are the newsgroup names that correspond to the labels?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYeL6zoJz13R"
      },
      "outputs": [],
      "source": [
        "target_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdWJolpB06Au"
      },
      "source": [
        "Create a data frame with columns for the documents, the newsgroup names, and their numerical labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaMsy1XAGLxc"
      },
      "outputs": [],
      "source": [
        "data_dict = {\n",
        "    'content': X,\n",
        "    'target': y,\n",
        "    'target_names': [target_names[target_index] for target_index in y]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data=data_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPjmhiwoIxW9"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGzpn4-rkApZ"
      },
      "outputs": [],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAFPzaPe1pN0"
      },
      "source": [
        "Re-use our `clean_data()` helper function from Module 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "na2bkOcFGter",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-6ce62b4e40acee5f",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def clean_data(text):\n",
        "    \"\"\"\n",
        "    Accepts a single text document and performs several regex substitutions in order to clean the document. \n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    text: string or object \n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    text: string or object\n",
        "    \"\"\"\n",
        "    \n",
        "    # order of operations - apply the expression from top to bottom\n",
        "    email_regex = \"From: \\S*@\\S*\\s?\"\n",
        "    non_alpha = '[^a-zA-Z]'\n",
        "    multi_white_spaces = \"[ ]{2,}\"\n",
        "    \n",
        "    text = re.sub(email_regex, \"\", text)\n",
        "    text = re.sub(non_alpha, ' ', text)\n",
        "    text = re.sub(multi_white_spaces, \" \", text)\n",
        "    \n",
        "    # apply case normalization \n",
        "    return text.lower().lstrip().rstrip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW5PRKbP10Q8"
      },
      "source": [
        "Clean the text string data and save it in a new column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQo1o1lSVKk4"
      },
      "outputs": [],
      "source": [
        "df[\"clean_data\"] = df[\"content\"].apply(clean_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4L_0TRh-VKk4"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUNbh3VlVKk6"
      },
      "source": [
        "### Create Tokens \n",
        "\n",
        "Before we can use the `gensim` library to create bag-of-words vectors in exactly the right way that the LDA model wants, we must first create tokens. \n",
        "\n",
        "Let's use spaCy to create some lemmas. But first let's initialize our multi-processing library `pandarallel` which will empower us to use the same dataframe that our data is stored in but be able to create tokens in parallel so as to save time.\n",
        "\n",
        "Here's the documentation for [**pandarallel**](https://github.com/nalepae/pandarallel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "okoXztIjVKk6"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'pandarallel' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\somer\\github\\DS-Unit-4-Sprint-1-NLP\\module4-topic-modeling\\DSPT15_414_Topic_Modeling_Lecture.ipynb Cell 37'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/somer/github/DS-Unit-4-Sprint-1-NLP/module4-topic-modeling/DSPT15_414_Topic_Modeling_Lecture.ipynb#ch0000036?line=0'>1</a>\u001b[0m \u001b[39m# we must initalize pandarallel before we can use it\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/somer/github/DS-Unit-4-Sprint-1-NLP/module4-topic-modeling/DSPT15_414_Topic_Modeling_Lecture.ipynb#ch0000036?line=1'>2</a>\u001b[0m pandarallel\u001b[39m.\u001b[39minitialize(progress_bar\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, nb_workers\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/somer/github/DS-Unit-4-Sprint-1-NLP/module4-topic-modeling/DSPT15_414_Topic_Modeling_Lecture.ipynb#ch0000036?line=2'>3</a>\u001b[0m \u001b[39m# so that the progress bars will work\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/somer/github/DS-Unit-4-Sprint-1-NLP/module4-topic-modeling/DSPT15_414_Topic_Modeling_Lecture.ipynb#ch0000036?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandarallel\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m progress_bars\n",
            "\u001b[1;31mNameError\u001b[0m: name 'pandarallel' is not defined"
          ]
        }
      ],
      "source": [
        "# we must initalize pandarallel before we can use it\n",
        "pandarallel.initialize(progress_bar=True, nb_workers=10)\n",
        "# so that the progress bars will work\n",
        "from pandarallel.utils import progress_bars\n",
        "progress_bars.is_notebook_lab = lambda : True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pndck8G6VKk7"
      },
      "outputs": [],
      "source": [
        "# load in our spaCy language model\n",
        "nlp = spacy.load(\"en_core_web_md\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EBPQXqEKE9P"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# create our tokens in the form of lemmas \n",
        "# parallel_apply takes about 2 min on Colab\n",
        "#df['lemmas'] = df['clean_data'].parallel_apply(lambda x: [token.lemma_ for token in nlp(x) if (token.is_stop != True) and (token.is_punct != True)])\n",
        "# parallel_map takes about 50 sec\n",
        "df['lemmas'] = df['clean_data'].parallel_map(lambda x: [token.lemma_ for token in nlp(x) if (token.is_stop != True) and (token.is_punct != True)])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02-ymM-5VKk7"
      },
      "source": [
        "### Take a look at our lemmas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5Ad8J11ersL"
      },
      "outputs": [],
      "source": [
        "df['clean_data'][5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MLNEA1IVKk7"
      },
      "outputs": [],
      "source": [
        "# print out the lemmas from the first article\n",
        "# note that some lemmas are only 1 character: you could modify the code above to not include lemmas that are only 1 or 2 characters\n",
        "df['lemmas'][5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHDnS31zVKk8"
      },
      "source": [
        "### Filter out low quality lemmas\n",
        "1 or 2 characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XryrcfNRVKk8"
      },
      "outputs": [],
      "source": [
        "def filter_lemmas(lemmas):\n",
        "    \"\"\"\n",
        "    Filter out any lemmas that are 2 characters or smaller\n",
        "    \"\"\"\n",
        "    return [lemma for lemma in lemmas if len(lemma) > 2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkLuMzP-VKk8"
      },
      "outputs": [],
      "source": [
        "# apply filter_lemmas\n",
        "# YOUR CODE HERE\n",
        "df[\"filtered_lemmas\"] = df[\"lemmas\"].parallel_map(filter_lemmas)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yo7bZGmhZNGx"
      },
      "outputs": [],
      "source": [
        "df[\"filtered_lemmas\"][5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eInhIUGr47um"
      },
      "source": [
        "Overwrite `lemmas` with `filtered_lemmas`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ov9Z9sUYZ1ZO"
      },
      "outputs": [],
      "source": [
        "df['lemmas'] = df[\"filtered_lemmas\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO6pb5RUVKk9"
      },
      "source": [
        "### The two main inputs to the `gensim` LDA topic model are the dictionary (`id2word`) and the `corpus`.\n",
        "- `id2word` is a special object created by `gensim` that keeps track of the mapping from text to numerical index, and the mapping from numerical index back to text <br>\n",
        "- The `corpus` is a specially formatted list containing information about each document. Though its format is different, is essentially equivalent to the document-term matrix that we used with `sklearn`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4Gex33Kbc6j"
      },
      "source": [
        "### Create the `id2word` dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1klqRpqtJxWc",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-79d38b90e5c6e38b",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# 1. Create Dictionary\n",
        "\n",
        "# 2. Term Document Frequency is (token id, token count) for each doc in the corpus\n",
        "\n",
        "# 3. Human readable format of corpus (term-frequency)\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "# 1. Create Dictionary from the corpus\n",
        "id2word = corpora.Dictionary(df['lemmas'] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHGUJftI58HT"
      },
      "outputs": [],
      "source": [
        "id2word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVbUm1jjBlYL"
      },
      "source": [
        "Use the `id2word.doc2bow()` method to process a first document into a list of word counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDIutZ_Gm5XO"
      },
      "outputs": [],
      "source": [
        "len(np.unique(df['lemmas'][5]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bkdg1KIvM4gv"
      },
      "outputs": [],
      "source": [
        "id2word.doc2bow(df['lemmas'][5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtZPMBuQCdjp"
      },
      "source": [
        "### Create the `corpus`\n",
        "by processing all the documents using a list comprehension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naC63SI2MfYV"
      },
      "outputs": [],
      "source": [
        "# 2. Term Document Frequency\n",
        "\n",
        "# We want a list of (token id, token count) for each doc in the corpus\n",
        "\n",
        "# Term Document Frequency -- different format, but equivalent in function to the document-term matrix we used in sklearn\n",
        "corpus = [id2word.doc2bow(doc_lemmas) for doc_lemmas in df['lemmas']]\n",
        "\n",
        "# corpus stores (token id, token count) for each doc in the corpus\n",
        "doc_id = 5\n",
        "corpus[doc_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxaFhjSoTsX6"
      },
      "outputs": [],
      "source": [
        "print(type(corpus))\n",
        "print(len(corpus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfYRe8lmLwM8"
      },
      "outputs": [],
      "source": [
        "\n",
        "### BEGIN SOLUTION\n",
        "\n",
        "# 3. Human readable format of corpus (term-frequency)\n",
        "[(id2word[word_id], word_count) for word_id, word_count in corpus[doc_id]]\n",
        "\n",
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJYODRc0VKk9"
      },
      "source": [
        "# Part 2: Estimate a Latent Dirichlet Allocation (LDA) Model with `gensim`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "of2bbspSVKk9"
      },
      "source": [
        " ### Train an LDA model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fasvjf0VLQ2a"
      },
      "outputs": [],
      "source": [
        "### This cell runs the single-processor version of the model (slower)\n",
        "# %%time\n",
        "# lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "#                                            id2word=id2word,\n",
        "#                                            num_topics=20, \n",
        "#                                            chunksize=100,\n",
        "#                                            passes=10,\n",
        "#                                            per_word_topics=True)\n",
        "# lda_model.save('lda_model.model')\n",
        "# # https://radimrehurek.com/gensim/models/ldamodel.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLQZOqcYVJOK"
      },
      "source": [
        "#### The multi-processor version of gensim's LDA model is `gensim.models.ldamulticore.LdaMulticore()`, <br> which runs faster than the single-core version `gensim.models.ldamodel.LdaModel()`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNjPCmaIVKk-"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "num_topics = 2\n",
        "### This cell runs the multi-processor version of the model (faster)\n",
        "lda_multicore_2_topics = gensim.models.ldamulticore.LdaMulticore(corpus=corpus,\n",
        "                                                        id2word=id2word,\n",
        "                                                        num_topics=num_topics, \n",
        "                                                        chunksize=100,\n",
        "                                                        passes=10,# runtime related parameter\n",
        "                                                        per_word_topics=True,\n",
        "                                                        workers=10, # runtime related parameter\n",
        "                                                        random_state=1234, \n",
        "                                                        iterations=20) # runtime related parameter\n",
        "\n",
        "num_topics = 6\n",
        "### This cell runs the multi-processor version of the model (faster)\n",
        "lda_multicore_6_topics = gensim.models.ldamulticore.LdaMulticore(corpus=corpus,\n",
        "                                                        id2word=id2word,\n",
        "                                                        num_topics=num_topics, \n",
        "                                                        chunksize=100,\n",
        "                                                        passes=10,# runtime related parameter\n",
        "                                                        per_word_topics=True,\n",
        "                                                        workers=10, # runtime related parameter\n",
        "                                                        random_state=1234, \n",
        "                                                        iterations=20) # runtime related parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJfS3bqYr24e"
      },
      "outputs": [],
      "source": [
        "dir(lda_multicore_6_topics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keEsz___VKk_"
      },
      "source": [
        "# Part 3: Interpret LDA results & Select the appropriate number of topics\n",
        "LDAvis provides an interactive visualization of the topics estimated using Latent Dirichlet Allocation (LDA).<br>\n",
        "We will use the `pyLDAvis` library which implements [LDAvis](https://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf) in `python`.<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8Chs_OtdclE"
      },
      "source": [
        "### 3.1 Visualizing the topics in our models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYXi480VLaHK"
      },
      "outputs": [],
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_multicore_2_topics, corpus, id2word)\n",
        "vis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5qjLqmXVKlA"
      },
      "outputs": [],
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_multicore_6_topics, corpus, id2word)\n",
        "vis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOC-5qivVKlA"
      },
      "source": [
        "###3.2 What is topic coherence?\n",
        "\n",
        "\n",
        "_Topic Coherence measures score a single topic by\n",
        "measuring the degree of semantic similarity between\n",
        "high scoring words in the topic._ <br>\n",
        "-- From \"Exploring Topic Coherence over many models and many topics\", by Keith Stevens, Philip Kegelmeyer, David Andrzejewski, and David Buttler, Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural\n",
        "Language Learning, pages 952–961, Jeju Island, Korea, 12–14 July 2012. \n",
        "c 2012 Association for Computational Linguistics\n",
        "\n",
        "_A set of statements or facts is said to be coherent, if they\n",
        "support each other. Thus, a coherent fact set can be interpreted\n",
        "in a context that covers all or most of the facts. An\n",
        "example of a coherent fact set is 'the game is a team sport',\n",
        "'the game is played with a ball', and 'the game demands great\n",
        "physical e\u000bfforts'_ -- from \"Exploring the Space of Topic Coherence Measures\", by Michael Roder, Andreas Both and Alexander Hinneburg https://dl.acm.org/doi/10.1145/2684822.2685324\n",
        "\n",
        "We expect that documents in a topic category should be coherent, i.e., they should relate to each other. The same is true for words within a topic.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXmKcnbucstY"
      },
      "source": [
        "### Let's tune the \"number of topics\" hyperparameter \n",
        "and choose the model with the best Topic Coherence score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjtXk8J3LaXC"
      },
      "outputs": [],
      "source": [
        "def compute_coherence_values(dictionary, corpus, texts, limit=None, start=None, step=None):\n",
        "    \"\"\"\n",
        "    Compute c_v coherence for various number of topics\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    dictionary : Gensim dictionary\n",
        "    corpus : Gensim corpus\n",
        "    texts : List of input texts\n",
        "    limit : Max num of topics\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    model_list : List of LDA topic models\n",
        "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
        "    \"\"\"\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        model = gensim.models.ldamulticore.LdaMulticore(corpus=corpus,\n",
        "                                                        id2word=id2word,\n",
        "                                                        num_topics=num_topics, \n",
        "                                                        chunksize=100,\n",
        "                                                        passes=10,\n",
        "                                                        random_state=1234,\n",
        "                                                        per_word_topics=True,\n",
        "                                                        workers=10)\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "\n",
        "    return model_list, coherence_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Da6R_R4AVKlB"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# takes about 3 min\n",
        "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=df['lemmas'], start=2, limit=10, step=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkdpAccnVKlB"
      },
      "outputs": [],
      "source": [
        "start=2; limit=10;  step=1;\n",
        "x = range(start, limit, step)\n",
        "\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.grid()\n",
        "plt.title(\"Coherence Score vs. Number of Topics\")\n",
        "plt.xticks(x)\n",
        "plt.plot(x, coherence_values, \"-o\")\n",
        "\n",
        "plt.xlabel(\"Num Topics\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjvsiYK6VKlC"
      },
      "source": [
        "### Select the \"best\" model as the one with the highest Topic Coherence Score\n",
        "\n",
        "Due to the probabilistic nature of this model, the modeling results can and usually do vary. Despite this, we will select 8 as the number of topics even if this particular model run doesn't show 8 as having the highest coherence score. Also, even if it doesn't, we  need to ask ourselves how many topics we actually want for our corpus. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjFf957DVKlC"
      },
      "outputs": [],
      "source": [
        "lda_trained_model = model_list[3] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8GtS65SVKlC"
      },
      "outputs": [],
      "source": [
        "lda_trained_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgTrsPLaShpm"
      },
      "source": [
        "### For each topic discovered by `gensim`, list the top ten words and their probabilities, "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J37mNQAlScUC"
      },
      "outputs": [],
      "source": [
        "pprint(lda_trained_model.print_topics())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbqJUL9nd-6x"
      },
      "source": [
        "###Visualize the topics in the best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h52Heg6aVKlD"
      },
      "outputs": [],
      "source": [
        "# visualize the topics \n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_trained_model, corpus, id2word)\n",
        "vis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuAJxqywVKlD"
      },
      "source": [
        "## Create a Topic Id --> Topic Name dictionary \n",
        "\n",
        "\n",
        "*****This is the \"human intervention\" step*****:<br>\n",
        "Edit `vis_topic_name_dict` with YOUR OWN names that YOU assign to the topics that `gensim` discovered.\n",
        "\n",
        "When populating your Topic Id --> Topic Name dictionary, use the index ordering as shown in the `pyLDAvis` tool. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKz7KQvwVKlD",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-777be73d0d1455f0",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# keys - use topic ids from pyLDAvis visualization \n",
        "# values - topic names that you create \n",
        "# save dictionary to `vis_topic_name_dict`\n",
        "###BEGIN SOLUTION\n",
        "# this is the results from a model run however these topic names might not make sense in future model runs\n",
        "# thus these names will likely have to be thought through during each lecture \n",
        "vis_topic_name_dict = {1:\"space program\", \n",
        "                       2:\"guns 'n crime\", \n",
        "                       3:\"mideast-politics-israel-religion\", \n",
        "                       4:\"armenian-turkish-politics\", \n",
        "                       5:\"electical power\"}\n",
        "###END SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5WBO7q_Z_tB"
      },
      "source": [
        "## Create a Topic Id lookup dictionary\n",
        "We'll use a function to map the `pyLDAvis` index ordering to the ordering used in gensim's trained LDA model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfH63ST9d1VB"
      },
      "outputs": [],
      "source": [
        "model_vis_tool_topic_id_lookup = vis.topic_coordinates.topics.to_dict()\n",
        "model_vis_tool_topic_id_lookup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynBqm_G9eH-E"
      },
      "outputs": [],
      "source": [
        "topic_id_lookup =  {v:k for k, v in model_vis_tool_topic_id_lookup.items()}\n",
        "topic_id_lookup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTrs_e35VKlE"
      },
      "outputs": [],
      "source": [
        "def get_topic_id_lookup_dict(vis, vis_topic_name_dict):\n",
        "    \"\"\"\n",
        "    Both the starting index and the ordering of topic ids bewteen the trained LDA model \n",
        "    and the viz tool are different. So we need to create a look up dictionary that maps \n",
        "    the correct association between topic ids from both sources. \n",
        "    \"\"\"\n",
        "    # value is order of topic ids according to pyLDAvis tool \n",
        "    # key is order of topic ids according to lda model\n",
        "    model_vis_tool_topic_id_lookup = vis.topic_coordinates.topics.to_dict()\n",
        "\n",
        "    # invert dictionary so that \n",
        "    # key is order of topic ids accoridng to pyLDAvis tool \n",
        "    # value is order of topic ids according to the trained lda model\n",
        "    topic_id_lookup =  {v:k for k, v in model_vis_tool_topic_id_lookup.items()}\n",
        "    \n",
        "    # iterate through topic_id_lookup and index vis_topic_name_dict using the keys \n",
        "    # in order to swap the viz topic ids in vis_topic_name_dict for the lda model topic ids \n",
        "    return {v:vis_topic_name_dict[k]  for k, v in topic_id_lookup.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9CYntf6VKlE",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-0718245fd125b36e",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "###BEGIN SOLUTION\n",
        "# now we have an updated topic id/name dict\n",
        "# the topic ids correspond to how the lda model has indexed the topics \n",
        "# now we can use this dictionary with lda model to label our docs \n",
        "topic_name_dict = get_topic_id_lookup_dict(vis, vis_topic_name_dict)\n",
        "###END SOLTUION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMX0VaB9VKlE"
      },
      "outputs": [],
      "source": [
        "topic_name_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvF8rSnsVKlF"
      },
      "source": [
        "## Use the LDA Model to Assign Each Document a Topic Name\n",
        "\n",
        "Now that we have a topic id/name look up dict that is aligned with the index ordering of the trained LDA model, we can move forward to giving each topic a topic name. \n",
        "\n",
        "This function iterates over the documents in a corpus and outputs a list with the topic ID -- according to the trained `lda_model` -- of the highest-probability topic for each document\n",
        "\n",
        "The function below has been given to you. However, you highly encouraged to read through it and make sure that you understand what it is doing each step of the way. In fact, a good way to do this is to copy and paste the code inside of the function into a new cell, comment out all the lines of code and line by line, uncomment the code and see the output. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TL1KiGfTVKlF"
      },
      "outputs": [],
      "source": [
        "def get_topic_ids_for_docs(lda_model, corpus):\n",
        "    \n",
        "    \"\"\"\n",
        "    Passes a Bag-of-Words vector into a trained LDA model in order to get the topic id of that document. \n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    lda_model: Gensim object\n",
        "        Must be a trained model \n",
        "        \n",
        "    corpus: nested lists of tuples, \n",
        "        i.e. [[(),(), ..., ()], [(),(), ..., ()], ..., [(),(), ..., ()]]\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    topic_id_list: list\n",
        "        Contains topic ids for all document vectors in corpus \n",
        "    \"\"\"\n",
        "    \n",
        "    # store topic ids for each document\n",
        "    doc_topic_ids = []\n",
        "\n",
        "    # iterature through the bow vectors for each doc\n",
        "    for doc_bow in corpus:\n",
        "        \n",
        "        # store the topic ids for the doc\n",
        "        topic_ids = []\n",
        "        # store the topic probabilities for the doc\n",
        "        topic_probs = []\n",
        "\n",
        "        # list of tuples\n",
        "        # each tuple has a topic id and the prob that the doc belongs to that topic \n",
        "        topic_id_prob_tuples = lda_trained_model.get_document_topics(doc_bow)\n",
        "        \n",
        "        # iterate through the topic id/prob pairs \n",
        "        for topic_id_prob in topic_id_prob_tuples:\n",
        "            \n",
        "            # index for topic id\n",
        "            topic_id = topic_id_prob[0]\n",
        "            # index for prob that doc belongs that the corresponding topic\n",
        "            topic_prob = topic_id_prob[1]\n",
        "\n",
        "            # store all topic ids for doc\n",
        "            topic_ids.append(topic_id)\n",
        "            # store all topic probs for doc\n",
        "            topic_probs.append(topic_prob)\n",
        "\n",
        "        # get index for largest prob score\n",
        "        max_topic_prob_ind = np.argmax(topic_probs)\n",
        "        # get corresponding topic id\n",
        "        max_prob_topic_id = topic_ids[max_topic_prob_ind]\n",
        "        # store topic id that had the highest prob for doc being a memebr of that topic\n",
        "        doc_topic_ids.append(max_prob_topic_id)\n",
        "        \n",
        "    return doc_topic_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFFT9C4AVKlF"
      },
      "outputs": [],
      "source": [
        "# get the topic id for each doc in the corpus \n",
        "topic_id_list = get_topic_ids_for_docs(lda_trained_model, corpus)\n",
        "\n",
        "# create a feature for document's topic id\n",
        "df[\"topic_id\"] = topic_id_list\n",
        "\n",
        "# iterate through the topic id and use the lookup table to assign each document with a topic name\n",
        "df[\"new_topic_name\"] = df[\"topic_id\"].apply(lambda topic_id: topic_name_dict[topic_id])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcfIEtYQVKlF"
      },
      "outputs": [],
      "source": [
        "# cool! so now all of our documents have topic ids and names \n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soHBOI5BMsdS"
      },
      "source": [
        "Using a boolean filter, or mask, you can slice out all articles of a certain topic, i.e. topic_id 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2L5fgW-QVKlG"
      },
      "outputs": [],
      "source": [
        "science_mask = df.topic_id == 3\n",
        "df[science_mask]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2RuVRieVKlG"
      },
      "source": [
        "-----\n",
        "\n",
        "## Knowledge is Power! What Can We Do with the Knowledge We Just Gained?\n",
        "\n",
        "Outside of this guided project (i.e. in your job) you may or may not have access to existing article topic names like we did with this data set. Meaning that you won't always have a point of reference to \"check your answers\". So let's explore two possible situations in which you might find yourself using Latent Dirichlet Allocation (LDA) Topic Modeling. \n",
        "\n",
        "### 1. You have access to existing document topic labels\n",
        "\n",
        "In this case, why would you bother with Topic Modeling? It could be the case that the current topic labels are actually not helpful for whatever task you're working on. For instance, in this guided project, our email dataset has topic names; however those topic labels are hierarchical, which may not suit your needs. So one option is generate new labels that do suit your needs (as we did here). \n",
        "\n",
        "### 2. Your corpus doesn't have any document topic labels\n",
        "\n",
        "In this case, you don't have any pre-existing topic labels. Maybe you work at Indeed or LinkedIn or Google and your job is to bring some structure to a huge collection of emails and messages that aren't labeled in any meaningful way and so it's difficult to just sort through these documents. This is a perfect use case of Topic Modeling. After you apply topic modeling, you'll then have organized your emails into broad categories and you can start structuring and then analyze your corpus, and maybe even build a supervised learning model to predict the topic of the document!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqY0KKxKQxbm"
      },
      "source": [
        "## Topic Modeling References\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKhe7u4IXr0y"
      },
      "source": [
        "- [Topic Modeling with Gensim](https://www.tutorialspoint.com/gensim/gensim_topic_modeling.htm) -- Brief Tutorial\n",
        "\n",
        "- [Gensim documentation](https://radimrehurek.com/gensim/index.html) -- look up gensim commands\n",
        "\n",
        "- [`pyLDAvis` documentation](https://pyldavis.readthedocs.io/en/latest/readme.html) -- package for visualizing LDA models\n",
        "\n",
        "- [Visualizing Topic Models](https://speakerdeck.com/bmabey/visualizing-topic-models) -- slides from Ben Mabey\n",
        "\n",
        "- [Exploring the Space of Topic Coherence Measures](https://dl.acm.org/doi/10.1145/2684822.2685324) -- Study of Topic Coherence measures\n",
        "\n",
        "- [Exploring Topic Coherence over many models and many topics](https://www.researchgate.net/publication/232242203_Exploring_Topic_Coherence_over_many_models_and_many_topics) -- Study of automated topic coherence measures"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Create Assignment",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "DSPT15_414_Topic_Modeling_Lecture.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "37f98fd59b33c666e4a497b057b58a4df7f85f0f750d2f64dd4fb9023e3cf7f6"
    },
    "kernelspec": {
      "display_name": "py37  (Python3)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
