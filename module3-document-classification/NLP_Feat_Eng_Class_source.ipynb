{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements should go at the top of all your notebooks\n",
    "# I know the curriculum has the import statements scatter everywhere\n",
    "# however this is how code should be maintained in a production environment (ie. industry)\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# Streamlined Steps for Feature Engineering Text Data\n",
    "\n",
    "1. Load text data\n",
    "2. Clean text data\n",
    "3. Tokenize text\n",
    "4. Vectorize text (i.e. create a document-term matrix)\n",
    "    - There is some nuance with steps 3 and 4 because you can treat them as two seperate steps or you can combine them into a single step. \n",
    "5. Data is ready to train a model !\n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load text data\n",
    "\n",
    "Create a function that does all the work necessary to load the data into dataframe. We want good, clean modular code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_data(subset = None, categories= None):\n",
    "    \"\"\"\n",
    "    Loads the 20newsgroups text data set into a datafarme for specified categories\n",
    "    LINK: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    Work with a subset of the data first to make the rest of the analysis as simple as possible. \n",
    "    Once all the code works as intended, then optionally increase the size of the data by selecting \n",
    "    more categories. \n",
    "    \n",
    "    Keep in mind that by using more than 2 categories in a classification task, \n",
    "    you are moving from a binary classification task to a multi-class classification task\n",
    "    \"\"\"\n",
    "    \n",
    "    if subset is None:\n",
    "        subset = \"train\"\n",
    "        \n",
    "    # load data from sklearn data api for news articles\n",
    "    data = fetch_20newsgroups(subset=subset, categories=categories)\n",
    "    \n",
    "    # move data into dataframe for ease in manipulating it\n",
    "    df = pd.DataFrame({\n",
    "            'content': data['data'],\n",
    "            'target': data['target'],\n",
    "            'target_names': [data['target_names'][i] for i in data['target']]})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this dataset has 20 doc categories, let's use just 2 for simplicity \n",
    "categories = ['alt.atheism',\n",
    "              'talk.religion.misc']\n",
    "\n",
    "df = load_text_data(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: mangoe@cs.umd.edu (Charley Wingate)\\nSub...</td>\n",
       "      <td>0</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: Re: There must be a creator! (Maybe)\\...</td>\n",
       "      <td>0</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: MANDTBACKA@FINABO.ABO.FI (Mats Andtbacka...</td>\n",
       "      <td>0</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: royc@rbdc.wsnc.org (Roy Crabtree)\\nSubje...</td>\n",
       "      <td>1</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: Re: \"Imaginary\" Friends - Info and Ex...</td>\n",
       "      <td>1</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  target  \\\n",
       "0  From: mangoe@cs.umd.edu (Charley Wingate)\\nSub...       0   \n",
       "1  Subject: Re: There must be a creator! (Maybe)\\...       0   \n",
       "2  From: MANDTBACKA@FINABO.ABO.FI (Mats Andtbacka...       0   \n",
       "3  From: royc@rbdc.wsnc.org (Roy Crabtree)\\nSubje...       1   \n",
       "4  Subject: Re: \"Imaginary\" Friends - Info and Ex...       1   \n",
       "\n",
       "         target_names  \n",
       "0         alt.atheism  \n",
       "1         alt.atheism  \n",
       "2         alt.atheism  \n",
       "3  talk.religion.misc  \n",
       "4  talk.religion.misc  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# great, we loaded the data into a dataframe (this step is done.)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 2. Clean text data\n",
    "\n",
    "Create a single function to clean the text data, all cleaning should occur in this single function. There are only 4 cleaning steps performed here, however you're free to make this as extensive a cleaning process as you wish. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# practice creating regular expression here: https://regexr.com/\n",
    "def clean_text_data(text):\n",
    "    \"\"\"\n",
    "    Clean data by using regex to remove unwanted text, like emails\n",
    "    \"\"\"\n",
    "    # remove the first and last white space at the tail ends of the text but the white spaces in between\n",
    "    # the white spaces in between are used to split the words during tokenization\n",
    "    text = text.strip()\n",
    "    \n",
    "    # remove emails from text by subtituting them with empty chars \n",
    "    re_exp_for_emails = 'From: \\S+@\\S+'\n",
    "    replace_with = ''\n",
    "    text = re.sub(re_exp_for_emails, replace_with, text)\n",
    "    \n",
    "    # Remove new line characters\n",
    "    re_exp_for_whitespaces = '\\\\n'\n",
    "    replace_with = ''\n",
    "    text = re.sub(re_exp_for_whitespaces, replace_with, text)  \n",
    "    \n",
    "    # Remove non-alphanumeric characters\n",
    "    re_exp_for_whitespaces = '[^0-9 a-zA-Z]+'\n",
    "    replace_with = ''\n",
    "    text = re.sub(re_exp_for_whitespaces, replace_with, text)  \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by have a single function for cleaning data, we only need to iterate through the rows ONCE!\n",
    "# this cuts down on run time, this is how efficient code is written. \n",
    "df['content'] = df['content'].apply(lambda row: clean_text_data(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Charley WingateSubject Benediktine Metaphysic...</td>\n",
       "      <td>0</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject Re There must be a creator Maybe Jim H...</td>\n",
       "      <td>0</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mats AndtbackaSubject Re An Anecdote about Is...</td>\n",
       "      <td>0</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Roy CrabtreeSubject Re A Message for you Mr P...</td>\n",
       "      <td>1</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject Re Imaginary Friends  Info and Experie...</td>\n",
       "      <td>1</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  target  \\\n",
       "0   Charley WingateSubject Benediktine Metaphysic...       0   \n",
       "1  Subject Re There must be a creator Maybe Jim H...       0   \n",
       "2   Mats AndtbackaSubject Re An Anecdote about Is...       0   \n",
       "3   Roy CrabtreeSubject Re A Message for you Mr P...       1   \n",
       "4  Subject Re Imaginary Friends  Info and Experie...       1   \n",
       "\n",
       "         target_names  \n",
       "0         alt.atheism  \n",
       "1         alt.atheism  \n",
       "2         alt.atheism  \n",
       "3  talk.religion.misc  \n",
       "4  talk.religion.misc  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## 3. Tokenize the text data\n",
    "\n",
    "Create a function that tokenizes our text. You are free to apply this function onto the text from outside of a vectorizer or from within a vectorizer. \n",
    "\n",
    "My advise is to first apply it outside of a vectorizer in order to sanity check that you are getting the results that you expect to see. Once you're sure that the tokenizer is performing as expected, pass the function into a vectorizer so that the tokenization happens internally within the vectoriers, the code is cleaner that way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the spacy model to help us tokenize the text\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(text, nlp):\n",
    "    \"\"\"\n",
    "    This function tokenizes our text documents. \n",
    "    \n",
    "    Note\n",
    "    ____\n",
    "    You are free to add as many filters as you want, here you have been presented with 3 filters \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    lemmas = []\n",
    "    \n",
    "    # by loading the text into our nlp model (i.e. word2vec) we can take advantage of the model's functionalities\n",
    "    # specifically, it has flags for whether or not a token is a stop word, punctuation, or a part-of-speech\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Something goes here :P\n",
    "    for token in doc: \n",
    "        # if token is not a stop word or punctuation or a pronoun, then take the lemma and save to list \n",
    "        if ((token.is_stop == False) and (token.is_punct == False)) and (token.pos_ != 'PRON'):\n",
    "            # save lowercase leema to list \n",
    "            lemmas.append(token.lemma_.lower())\n",
    "    \n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"lemmas\"] = df['content'].apply(lambda row: custom_tokenizer(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [ , charley, wingatesubject, benediktine, meta...\n",
       "1    [subject, creator, maybe, jim, halatreplyto, h...\n",
       "2    [ , mats, andtbackasubject, anecdote, islaminr...\n",
       "3    [ , roy, crabtreesubject, message, mr, preside...\n",
       "4    [subject, imaginary, friend,  , info, experien...\n",
       "Name: lemmas, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cool - these results look like what I expected to see (expect for those white spaces...)\n",
    "df[\"lemmas\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## 4. Vectorizer Text\n",
    "\n",
    "**The output of a vectorizer is always a term-doc matrix.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = df['content']\n",
    "\n",
    "# pass in our custom_tokenizer as an argument\n",
    "tfidf = TfidfVectorizer(tokenizer=custom_tokenizer)\n",
    "term_doc_matrix = tfidf.fit_transform(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<857x28793 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 100948 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output is stored in a sparse matrix\n",
    "term_doc_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## 5. Document classification\n",
    "\n",
    "This is the step where we train a model. There are many ways to do this, we could simply pass the data into a model and call fit. However, let's be a little more thorough. \n",
    "\n",
    "Now that we are clear on the order of operations, we can build out the gridsearch code (and pipeline) that will train our document classifer. \n",
    "\n",
    "We are going to combine all the steps that we completed so far into a single cell of code -- nice and concise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1. Load data\n",
    "categories = ['alt.atheism',\n",
    "              'talk.religion.misc']\n",
    "df = load_text_data(categories)\n",
    "\n",
    "#### 2. Clean data\n",
    "df['clean_text'] = df['content'].apply(lambda row: clean_text_data(row))\n",
    "\n",
    "#### 3 & 4. Create vectorizer using custom tokenizer \n",
    "vect = TfidfVectorizer(tokenizer=custom_tokenizer)\n",
    "\n",
    "#### 5. Train model \n",
    "\n",
    "# instantiate model\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# fill out pipeline\n",
    "pipe = Pipeline([\n",
    "                 #Vectorizer\n",
    "                 ('vect', vect),\n",
    "                 # Classifier\n",
    "                 ('clf', rfc)\n",
    "                ])\n",
    "\n",
    "# create parameter dict with params for both the vectorizer and model\n",
    "parameters = {\n",
    "#     'vect__max_features': (500,1000),\n",
    "#     'clf__n_estimators':(5, 10,),\n",
    "    'clf__max_depth':(15,20)\n",
    "}\n",
    "\n",
    "# pass in the pipeline and parameters dict to gridsearch\n",
    "grid_search = GridSearchCV(pipe, \n",
    "                           parameters, \n",
    "                           cv=3, \n",
    "                           n_jobs=10, # change to suit your resources (i.e. whatever cores you have available)\n",
    "                           verbose=1)\n",
    "\n",
    "# because we are using a pipeline, we can pass in raw data! \n",
    "grid_search.fit(df.clean_text.iloc[:200], df.target.iloc[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Notes\n",
    "\n",
    "Because we are including the vectorizer in the pipeline (which is passed into the gridsearch) this means that for every unique combination of parameters that the girdsearch uses to build a model, it is also re-tokenizing the text and re-vectorizing it each time. This is a time-consuming process, especially if you have a large data set and are fitting a lot of parameters with a lot of parameter values -- so include parameters wisely!\n",
    "\n",
    "Also, Unit 4 is introducing you into data science at scale with NLP and with Deep Learning. This is why at some point we start doing data science in the cloud, i.e. AWS EC2 instance with 32 or even 64 cores or use a GPU instance for deep learning. \n",
    "\n",
    "On option to shorten the gridsearch run time is to exclude the vectorizer in the gridsearch. I'm not going to include an example of this because a side-by-side example of what I'm walking about already exist in the Warm Up for the Topic Modeling lecture notebook. However, I hope that this notebook helps streamline the proceedure of document classification in your mind. So now you could refer back to that lecture notebook and, hopefully, be able to read through those warm up examples with a bit more ease. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Create NLP Feature Engineering Pipeline Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class NLP_feat_eng_pipeline(object):\n",
    "    \n",
    "    def __init__(self, df = None, nlp = None, vectorizer=\"tfidf\"):\n",
    "        \"\"\"\n",
    "        This class serves as a feature engineering pipeline for text data.\n",
    "        Providing functionality to load, clean, tokenize, and vectorize \n",
    "        text data for the use in NLP tasks, such as predictive analytics.  \n",
    "        \n",
    "        Parmaters\n",
    "        ---------\n",
    "        df: pandas dataframe\n",
    "            Option to provide text data in a dataframe instead of using this class to load the data.\n",
    "        \"\"\"\n",
    "        \n",
    "        # data set already in a dataframe \n",
    "        self.df = df \n",
    "        \n",
    "        # Spacy's Word2Vec word embedding model \n",
    "        self.nlp = nlp\n",
    "        \n",
    "        # term document matrix\n",
    "        self.term_doc_matrix = None\n",
    "             \n",
    "        # make sure that the user selects a valid vectorizer \n",
    "        if self.vectorizer not in [\"tfidf\", \"bow\", \"w2v\"]:\n",
    "            msg = \"{} is an invalid selection for vectorizer.\".format(self.vectorizer)\n",
    "            raise MyException(msg)\n",
    "            \n",
    "        self.vectorizer=vectorizer\n",
    "\n",
    "    def load_20newsgroups_data(self, subset = None, categories= None):\n",
    "        \"\"\"\n",
    "        Loads the 20newsgroups text data set into a datafarme for specified categories\n",
    "        LINK: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        Work with a subset of the data first to make the rest of the analysis as simple as possible. \n",
    "        Once all the code works as intended, then optionally increase the size of the data by selecting \n",
    "        more categories. \n",
    "\n",
    "        Keep in mind that by using more than 2 categories in a classification task, \n",
    "        you are moving from a binary classification task to a multi-class classification task\n",
    "        \"\"\"\n",
    "\n",
    "        if subset is None:\n",
    "            subset = \"train\"\n",
    "            \n",
    "        if categories is None:\n",
    "            categories = [\"sci.electronics\", \n",
    "                          \"sci.space\"]\n",
    "\n",
    "        # load data from sklearn data api for news articles\n",
    "        data = fetch_20newsgroups(subset=subset, categories=categories)\n",
    "\n",
    "        # move data into dataframe for ease in manipulating it\n",
    "        df = pd.DataFrame({\n",
    "                'content': data['data'],\n",
    "                'target': data['target'],\n",
    "                'target_names': [data['target_names'][i] for i in data['target']]})\n",
    "\n",
    "        self.df = df\n",
    "        \n",
    "        \n",
    "    # practice creating regular expression here: https://regexr.com/\n",
    "    def _clean_text_data(self, text):\n",
    "        \"\"\"\n",
    "        Clean data by using regex to remove unwanted text, like emails\n",
    "        \n",
    "        Note\n",
    "        ----\n",
    "        This method is not intended to be used directly. It is intended to be call \n",
    "        within the clean_text method.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        text: string \n",
    "            A single document \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        text: string \n",
    "            A single document that has been cleaned \n",
    "        \"\"\"\n",
    "        # remove the first and last white space at the tail ends of the text but the white spaces in between\n",
    "        # the white spaces in between are used to split the words during tokenization\n",
    "        text = text.strip()\n",
    "\n",
    "        # remove emails from text by subtituting them with empty chars \n",
    "        re_exp_for_emails = 'From: \\S+@\\S+'\n",
    "        replace_with = ''\n",
    "        text = re.sub(re_exp_for_emails, replace_with, text)\n",
    "\n",
    "        # Remove new line characters\n",
    "        re_exp_for_whitespaces = '\\\\n'\n",
    "        replace_with = ''\n",
    "        text = re.sub(re_exp_for_whitespaces, replace_with, text)  \n",
    "\n",
    "        # Remove non-alphanumeric characters\n",
    "        re_exp_for_whitespaces = '[^0-9 a-zA-Z]+'\n",
    "        replace_with = ''\n",
    "        text = re.sub(re_exp_for_whitespaces, replace_with, text)  \n",
    "\n",
    "        return text\n",
    "    \n",
    "    def clean_text(self):\n",
    "        \"\"\"\n",
    "        Clean data by using regex to remove unwanted text, like emails. \n",
    "        \n",
    "        This method calls self._clean_text_data in a loop. Use to method for cleanng more than a single document. \n",
    "        \"\"\"\n",
    "        \n",
    "        self.df['content'] = self.df['content'].apply(lambda row: self._clean_text_data(row))\n",
    "        \n",
    "        \n",
    "    def tokenizer(self, text):\n",
    "        \"\"\"\n",
    "        This function tokenizes our text documents. It filters out certain tokens (like stop words)\n",
    "        and keeps lower case lemmas of words. \n",
    "\n",
    "        Note\n",
    "        ____\n",
    "        You are free to add as many filters as you want, here you have been presented with 3 filters\n",
    "            - stop words\n",
    "            - punctuation\n",
    "            - white space\n",
    "        \"\"\"\n",
    "\n",
    "        lemmas = []\n",
    "\n",
    "        # by loading the text into our nlp model (i.e. word2vec) we can take advantage of the model's functionalities\n",
    "        # specifically, it has flags for whether or not a token is a stop word, punctuation, or a part-of-speech\n",
    "        doc = self.nlp(text)\n",
    "\n",
    "        # Something goes here :P\n",
    "        for token in doc: \n",
    "            # if token is not a stop word or punctuation or a pronoun, then take the lemma and save to list \n",
    "            if ((token.is_stop == False) and (token.is_punct == False)) and (token.is_space == False):\n",
    "                # save lowercase leema to list \n",
    "                lemmas.append(token.lemma_.lower())\n",
    "\n",
    "        return lemmas\n",
    "    \n",
    "    def get_tokens(self):\n",
    "        \"\"\"\n",
    "        Tokenize documents and save to a new feature \n",
    "        \n",
    "        This method calls self.tokenizer in a loop. Use this method for tokenizing more than a single document. \n",
    "        \"\"\"\n",
    "            \n",
    "        self.df[\"tokens\"] = self.df['content'].apply(lambda doc: self.tokenizer(doc))\n",
    "        \n",
    "        \n",
    "    def vectorize(self):\n",
    "        \"\"\"\n",
    "        Use a vectorizer to create a term-docuemnt matrix. \n",
    "        It is this term-document matrix that will be in ML model friendly data format.\n",
    "        \n",
    "        Note\n",
    "        ----\n",
    "        This method will also perform tokenization, so no need to call self.get_tokens\n",
    "        if using this method. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        vectorizer: string \n",
    "            Valid options are [\"tfidf\", \"bow\", \"w2v\"]\n",
    "            tfidf: TfidfVectorizer\n",
    "            bow: Bag-of-Words (i.e. CountVectorizer)\n",
    "            w2v: Word2Vec \n",
    "        \"\"\"\n",
    "                    \n",
    "        if self.vectorizer == \"tfidf\":\n",
    "            vect = TfidfVectorizer(tokenizer=self.tokenizer)\n",
    "            \n",
    "        elif self.vectorizer == \"bow\":\n",
    "            vect = CountVectorizer(tokenizer=self.tokenizer)            \n",
    "            \n",
    "        # use a term count vectorizer \n",
    "        if self.vectorizer  in [\"tfidf\", \"bow\"]:\n",
    "            self.term_doc_matrix = vect.fit_transform(self.df[\"content\"])\n",
    "            \n",
    "        # use word embedding (i.e. word2vec)\n",
    "        else:\n",
    "            self.term_doc_matrix = self.df[\"content\"].apply(lambda doc: self.nlp(doc).vector)\n",
    "        \n",
    "    def get_term_doc_matrix(self):\n",
    "        \"\"\"\n",
    "        Convience function.\n",
    "        Use this function to load, clean, vectorize a dataset, and return the doc-term matrix.\n",
    "        \"\"\"\n",
    "        \n",
    "        # load data \n",
    "        #self.load_20newsgroups_data()\n",
    "        \n",
    "        # clean data\n",
    "        self.clean_text()\n",
    "        \n",
    "        # tokenize and vectorize data\n",
    "        self.vectorize()\n",
    "        \n",
    "        return self.term_doc_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Deploy the Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the spacy model to help us tokenize the text\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_pipe = NLP_feat_eng_pipeline(nlp=nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  target  \\\n",
       "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...       7   \n",
       "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...       4   \n",
       "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...       4   \n",
       "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...       1   \n",
       "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...      14   \n",
       "\n",
       "            target_names  \n",
       "0              rec.autos  \n",
       "1  comp.sys.mac.hardware  \n",
       "2  comp.sys.mac.hardware  \n",
       "3          comp.graphics  \n",
       "4              sci.space  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_pipe.load_20newsgroups_data()\n",
    "nlp_pipe.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wheres my thingSubject WHAT car is thisNntpPo...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Guy KuoSubject SI Clock Poll  Final CallSumma...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thomas E WillisSubject PB questionsOrganizati...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Joe GreenSubject Re Weitek P9000 Organization...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jonathan McDowellSubject Re Shuttle Launch Qu...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  target  \\\n",
       "0   wheres my thingSubject WHAT car is thisNntpPo...       7   \n",
       "1   Guy KuoSubject SI Clock Poll  Final CallSumma...       4   \n",
       "2   Thomas E WillisSubject PB questionsOrganizati...       4   \n",
       "3   Joe GreenSubject Re Weitek P9000 Organization...       1   \n",
       "4   Jonathan McDowellSubject Re Shuttle Launch Qu...      14   \n",
       "\n",
       "            target_names  \n",
       "0              rec.autos  \n",
       "1  comp.sys.mac.hardware  \n",
       "2  comp.sys.mac.hardware  \n",
       "3          comp.graphics  \n",
       "4              sci.space  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_pipe.clean_text()\n",
    "nlp_pipe.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_pipe.df = nlp_pipe.df.sample(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8246</th>\n",
       "      <td>Subject RADAR DETECTOR Whistler XKKaOrganizati...</td>\n",
       "      <td>6</td>\n",
       "      <td>misc.forsale</td>\n",
       "      <td>[subject, radar, detector, whistler, xkkaorgan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6611</th>\n",
       "      <td>Gregory LehmanSubject Looking for drawing pac...</td>\n",
       "      <td>5</td>\n",
       "      <td>comp.windows.x</td>\n",
       "      <td>[gregory, lehmansubject, look, draw, packageso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3479</th>\n",
       "      <td>Subject Re A to D hardware for a PCArticleID a...</td>\n",
       "      <td>12</td>\n",
       "      <td>sci.electronics</td>\n",
       "      <td>[subject, d, hardware, pcarticleid, almaden199...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5581</th>\n",
       "      <td>PatSubject Plutonium based Nuclear Power plan...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>[patsubject, plutonium, base, nuclear, power, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11213</th>\n",
       "      <td>David BryantSubject Re GUI toolkit for the Su...</td>\n",
       "      <td>5</td>\n",
       "      <td>comp.windows.x</td>\n",
       "      <td>[david, bryantsubject, gui, toolkit, sun, spar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content  target  \\\n",
       "8246   Subject RADAR DETECTOR Whistler XKKaOrganizati...       6   \n",
       "6611    Gregory LehmanSubject Looking for drawing pac...       5   \n",
       "3479   Subject Re A to D hardware for a PCArticleID a...      12   \n",
       "5581    PatSubject Plutonium based Nuclear Power plan...      14   \n",
       "11213   David BryantSubject Re GUI toolkit for the Su...       5   \n",
       "\n",
       "          target_names                                             tokens  \n",
       "8246      misc.forsale  [subject, radar, detector, whistler, xkkaorgan...  \n",
       "6611    comp.windows.x  [gregory, lehmansubject, look, draw, packageso...  \n",
       "3479   sci.electronics  [subject, d, hardware, pcarticleid, almaden199...  \n",
       "5581         sci.space  [patsubject, plutonium, base, nuclear, power, ...  \n",
       "11213   comp.windows.x  [david, bryantsubject, gui, toolkit, sun, spar...  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_pipe.get_tokens()\n",
    "nlp_pipe.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_pipe.vectorize(vectorizer=\"w2v\")\n",
    "dtm = nlp_pipe.term_doc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8246     [-0.024148304, 0.19941829, 0.021181103, -0.059...\n",
       "6611     [0.022570314, 0.087968335, -0.16035953, -0.029...\n",
       "3479     [-0.012966986, 0.08954542, -0.07790943, -0.008...\n",
       "5581     [-0.06252756, 0.05107255, -0.03644632, 0.02560...\n",
       "11213    [-0.034638315, 0.10063207, -0.09123432, -0.010...\n",
       "                               ...                        \n",
       "10264    [-0.013094424, 0.110703565, -0.14540501, -0.05...\n",
       "1703     [-0.016310517, 0.031511243, -0.042582467, -0.0...\n",
       "7933     [0.046159443, 0.09234426, -0.16290684, 0.01605...\n",
       "2360     [-0.04317045, 0.11875365, -0.1119552, -0.07151...\n",
       "1162     [-0.050491374, 0.11384664, -0.05856432, -0.061...\n",
       "Name: content, Length: 100, dtype: object"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output of ETL pipeline is model ready data \n",
    "dtm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37  (Python3)",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
